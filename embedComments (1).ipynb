{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5237567f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch1RUyoCaQBL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA1JDINHaUO9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy<2.0\" \"transformers<4.41\" accelerate sentencepiece packaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt4xcfsIat3t"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Pre-flight check for common numpy/transformers compatibility issues\n",
        "import sys\n",
        "from packaging import version\n",
        "try:\n",
        "    from transformers import __version__ as transformers_version\n",
        "    if version.parse(np.__version__) >= version.parse(\"2.0.0\") and \\\n",
        "       version.parse(transformers_version) < version.parse(\"4.41.0\"):\n",
        "        print(\"=\"*80, file=sys.stderr)\n",
        "        print(\"ERROR: Incompatible library versions detected!\", file=sys.stderr)\n",
        "        print(f\"Numpy version {np.__version__} is not compatible with Transformers version {transformers_version}.\", file=sys.stderr)\n",
        "        print(\"\\nTo fix, run ONE of the following commands in your notebook and RESTART the runtime:\", file=sys.stderr)\n",
        "        print(\"1. Downgrade NumPy: !pip install --upgrade \\\"numpy<2.0\\\"\", file=sys.stderr)\n",
        "        print(\"2. Upgrade Transformers: !pip install --upgrade transformers accelerate\", file=sys.stderr)\n",
        "        print(\"=\"*80, file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    # If transformers isn't installed, the script will fail on the next import anyway.\n",
        "    # This check is specifically for the version mismatch.\n",
        "    pass\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import List, Dict, Any\n",
        "import gc\n",
        "import time\n",
        "import psutil\n",
        "# Optional: speed/precision helpers\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Skip Drive import when not in Colab; this keeps the script runnable locally\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except ModuleNotFoundError:\n",
        "    drive = None  # noqa: F401\n",
        "\n",
        "class BatchOptimizer:\n",
        "    \"\"\"Dynamic batch size optimizer that adjusts based on GPU memory usage\"\"\"\n",
        "\n",
        "    def __init__(self, initial_batch_size: int = 8, max_batch_size: int = 256,\n",
        "                 target_memory_utilization: float = 0.85, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the batch optimizer\n",
        "\n",
        "        Args:\n",
        "            initial_batch_size: Starting batch size\n",
        "            max_batch_size: Maximum allowed batch size\n",
        "            target_memory_utilization: Target GPU memory utilization (0.0-1.0)\n",
        "            device: Device type (\"cuda\", \"cpu\", \"mps\")\n",
        "        \"\"\"\n",
        "        self.current_batch_size = initial_batch_size\n",
        "        self.initial_batch_size = initial_batch_size\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.target_memory_utilization = target_memory_utilization\n",
        "        self.device = device\n",
        "\n",
        "        # Tracking variables\n",
        "        self.successful_batches = 0\n",
        "        self.memory_usage_history = []\n",
        "        self.batch_size_history = []\n",
        "        self.last_oom = False\n",
        "        self.adjustment_cooldown = 0\n",
        "\n",
        "        print(f\"BatchOptimizer initialized - Initial batch size: {self.current_batch_size}\")\n",
        "\n",
        "    def get_gpu_memory_info(self) -> Dict[str, float]:\n",
        "        \"\"\"Get current GPU memory usage information\"\"\"\n",
        "        if self.device != \"cuda\" or not torch.cuda.is_available():\n",
        "            return {\"used\": 0.0, \"total\": 1.0, \"utilization\": 0.0}\n",
        "\n",
        "        try:\n",
        "            # Get memory info for the current device\n",
        "            memory_used = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
        "            utilization = memory_used / memory_total if memory_total > 0 else 0.0\n",
        "\n",
        "            return {\n",
        "                \"used\": memory_used,\n",
        "                \"total\": memory_total,\n",
        "                \"utilization\": utilization\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting GPU memory info: {e}\")\n",
        "            return {\"used\": 0.0, \"total\": 1.0, \"utilization\": 0.0}\n",
        "\n",
        "    def should_increase_batch_size(self, memory_info: Dict[str, float]) -> bool:\n",
        "        \"\"\"Determine if batch size should be increased\"\"\"\n",
        "        if self.last_oom or self.adjustment_cooldown > 0:\n",
        "            return False\n",
        "\n",
        "        # Only increase if we have successful batches and low memory usage\n",
        "        return (self.successful_batches >= 3 and\n",
        "                memory_info[\"utilization\"] < self.target_memory_utilization - 0.1 and\n",
        "                self.current_batch_size < self.max_batch_size)\n",
        "\n",
        "    def should_decrease_batch_size(self, memory_info: Dict[str, float]) -> bool:\n",
        "        \"\"\"Determine if batch size should be decreased\"\"\"\n",
        "        # Decrease if memory usage is too high\n",
        "        return memory_info[\"utilization\"] > self.target_memory_utilization + 0.05\n",
        "\n",
        "    def adjust_batch_size(self, memory_info: Dict[str, float]) -> int:\n",
        "        \"\"\"Adjust batch size based on current memory usage\"\"\"\n",
        "        old_batch_size = self.current_batch_size\n",
        "\n",
        "        if self.adjustment_cooldown > 0:\n",
        "            self.adjustment_cooldown -= 1\n",
        "            return self.current_batch_size\n",
        "\n",
        "        if self.should_increase_batch_size(memory_info):\n",
        "            # Increase batch size gradually\n",
        "            increase_factor = 1.5 if memory_info[\"utilization\"] < 0.6 else 1.2\n",
        "            new_batch_size = min(int(self.current_batch_size * increase_factor), self.max_batch_size)\n",
        "            self.current_batch_size = new_batch_size\n",
        "            self.adjustment_cooldown = 2  # Wait 2 batches before next adjustment\n",
        "            print(f\"ðŸ“ˆ Increased batch size: {old_batch_size} â†’ {new_batch_size} (GPU: {memory_info['utilization']:.1%})\")\n",
        "\n",
        "        elif self.should_decrease_batch_size(memory_info):\n",
        "            # Decrease batch size more aggressively\n",
        "            decrease_factor = 0.7 if memory_info[\"utilization\"] > 0.95 else 0.8\n",
        "            new_batch_size = max(int(self.current_batch_size * decrease_factor), 1)\n",
        "            self.current_batch_size = new_batch_size\n",
        "            self.adjustment_cooldown = 3  # Wait longer after decreasing\n",
        "            print(f\"ðŸ“‰ Decreased batch size: {old_batch_size} â†’ {new_batch_size} (GPU: {memory_info['utilization']:.1%})\")\n",
        "\n",
        "        return self.current_batch_size\n",
        "\n",
        "    def handle_oom_error(self):\n",
        "        \"\"\"Handle out-of-memory error by reducing batch size\"\"\"\n",
        "        old_batch_size = self.current_batch_size\n",
        "        self.current_batch_size = max(1, self.current_batch_size // 2)\n",
        "        self.last_oom = True\n",
        "        self.adjustment_cooldown = 5  # Wait longer after OOM\n",
        "        self.successful_batches = 0 # Reset confidence after OOM\n",
        "        print(f\"ðŸ’¥ OOM Error! Reduced batch size: {old_batch_size} â†’ {self.current_batch_size}\")\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    def record_successful_batch(self, memory_info: Dict[str, float]):\n",
        "        \"\"\"Record a successful batch processing\"\"\"\n",
        "        self.successful_batches += 1\n",
        "        self.last_oom = False\n",
        "        self.memory_usage_history.append(memory_info[\"utilization\"])\n",
        "        self.batch_size_history.append(self.current_batch_size)\n",
        "\n",
        "        # Keep only recent history\n",
        "        if len(self.memory_usage_history) > 50:\n",
        "            self.memory_usage_history = self.memory_usage_history[-50:]\n",
        "            self.batch_size_history = self.batch_size_history[-50:]\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get optimization statistics\"\"\"\n",
        "        if not self.memory_usage_history:\n",
        "            return {\"status\": \"No data available\"}\n",
        "\n",
        "        return {\n",
        "            \"current_batch_size\": self.current_batch_size,\n",
        "            \"initial_batch_size\": self.initial_batch_size,\n",
        "            \"successful_batches\": self.successful_batches,\n",
        "            \"avg_memory_utilization\": np.mean(self.memory_usage_history),\n",
        "            \"max_memory_utilization\": np.max(self.memory_usage_history),\n",
        "            \"batch_size_range\": f\"{min(self.batch_size_history)}-{max(self.batch_size_history)}\"\n",
        "        }\n",
        "\n",
        "# This class handles loading a transformer model and embedding comment texts\n",
        "class CommentEmbedder:\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"intfloat/multilingual-e5-small\",\n",
        "                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                 batch_size: int = 8,\n",
        "                 use_fp16: bool = True,\n",
        "                 compile_model: bool = True,\n",
        "                 optimize_batch_size: bool = True,\n",
        "                 max_batch_size: int = 256):\n",
        "        \"\"\"Initialise tokenizer/model with optional half-precision and Torch compile.\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model id\n",
        "            device: \"cuda\" | \"cpu\" | \"mps\"\n",
        "            batch_size: initial batch size for embedding\n",
        "            use_fp16: Cast model to float16 when running on GPU for speed & memory\n",
        "            compile_model: Run `torch.compile` (PyTorch â‰¥2.0) for kernel fusion\n",
        "            optimize_batch_size: Enable dynamic batch size optimization\n",
        "            max_batch_size: Maximum allowed batch size for optimization\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Initialize batch optimizer\n",
        "        self.optimize_batch_size = optimize_batch_size and device == \"cuda\"\n",
        "        if self.optimize_batch_size:\n",
        "            self.batch_optimizer = BatchOptimizer(\n",
        "                initial_batch_size=batch_size,\n",
        "                max_batch_size=max_batch_size,\n",
        "                device=device\n",
        "            )\n",
        "            self.batch_size = self.batch_optimizer.current_batch_size\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "            self.batch_optimizer = None\n",
        "\n",
        "        # Load tokenizer / model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Move to device & precision\n",
        "        if self.device == \"cuda\":\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True  # potentially faster\n",
        "            if use_fp16:\n",
        "                self.model = self.model.half()\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Optionally compile (PyTorch 2.x)\n",
        "        if compile_model and version.parse(torch.__version__) >= version.parse(\"2.0\") and self.device == \"cuda\":\n",
        "            try:\n",
        "                self.model = torch.compile(self.model)\n",
        "                print(\"Model compiled with torch.compile()\")\n",
        "            except Exception as compile_err:  # pragma: no cover\n",
        "                print(f\"torch.compile failed: {compile_err}. Continuing without compilation.\")\n",
        "\n",
        "        print(f\"Model loaded on {self.device} (fp16={use_fp16})\")\n",
        "        if self.device == \"cuda\":\n",
        "            total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "            print(f\"GPU Memory: {total_mem:.2f} GB\")\n",
        "            if self.optimize_batch_size:\n",
        "                print(\"ðŸš€ Dynamic batch size optimization enabled\")\n",
        "\n",
        "    def prepare_comment(self, text: str) -> str:\n",
        "        \"\"\"Prepare comment text following E5 format\"\"\"\n",
        "        # Clean and format text for the E5 model (prefix with 'query: ')\n",
        "        text = str(text).strip()\n",
        "        if not text:\n",
        "            return \"query: empty comment\"\n",
        "        return f\"query: {text}\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Embed a single batch of texts.\"\"\"\n",
        "        # This method will now raise torch.cuda.OutOfMemoryError on failure,\n",
        "        # to be handled by the calling function.\n",
        "\n",
        "        # Prepare texts for the model\n",
        "        prepared_texts = [self.prepare_comment(text) for text in texts]\n",
        "\n",
        "        # Tokenize the batch of texts\n",
        "        encoded = self.tokenizer(\n",
        "            prepared_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Get model outputs (embeddings)\n",
        "        outputs = self.model(**encoded)\n",
        "        # Use the CLS token embedding as the sentence embedding\n",
        "        embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "        # Normalize embeddings to unit length\n",
        "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "        # Record successful batch if optimizing\n",
        "        if self.batch_optimizer:\n",
        "            memory_info_after = self.batch_optimizer.get_gpu_memory_info()\n",
        "            self.batch_optimizer.record_successful_batch(memory_info_after)\n",
        "\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "    def process_file(self, input_path: Path, output_path: Path = None) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single JSON file with dynamic batch optimization and checkpointing.\"\"\"\n",
        "        # Set output path if not provided\n",
        "        if output_path is None:\n",
        "            output_path = input_path.parent / f\"{input_path.stem}_embeddings.npz\"\n",
        "\n",
        "        # Temporary directory for checkpointing batches\n",
        "        tmp_dir = output_path.parent / f\".tmp_{input_path.stem}\"\n",
        "        tmp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load comments from JSON file\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Handle both array of strings and array of objects\n",
        "        comments = []\n",
        "        comment_ids = []\n",
        "\n",
        "        # Extract comments and their IDs (if present)\n",
        "        for idx, item in enumerate(data):\n",
        "            if isinstance(item, dict) and 'comment' in item:\n",
        "                # Handle object format\n",
        "                comments.append(item['comment'])\n",
        "                comment_ids.append(item.get('id', idx))\n",
        "            elif isinstance(item, str):\n",
        "                # Handle string format\n",
        "                comments.append(item)\n",
        "                comment_ids.append(idx)\n",
        "\n",
        "        if not comments:\n",
        "            # Clean up temp dir if no comments are found\n",
        "            if tmp_dir.exists():\n",
        "                shutil.rmtree(tmp_dir)\n",
        "            raise ValueError(f\"No valid comments found in {input_path}\")\n",
        "\n",
        "        print(f\"\\nProcessing {len(comments)} comments from {input_path.name}\")\n",
        "\n",
        "        # --- Resume logic ---\n",
        "        all_embeddings = []\n",
        "        processed_count = 0\n",
        "        batch_counter = 0\n",
        "\n",
        "        # Discover and load existing batches to resume\n",
        "        try:\n",
        "            # Sort by batch number to ensure correct order\n",
        "            existing_batches = sorted(\n",
        "                tmp_dir.glob(\"batch_*.npz\"),\n",
        "                key=lambda p: int(p.stem.split('_')[1])\n",
        "            )\n",
        "\n",
        "            if existing_batches:\n",
        "                print(f\"Resuming from {len(existing_batches)} completed batches...\")\n",
        "                for batch_file in existing_batches:\n",
        "                    with np.load(batch_file) as batch_data:\n",
        "                        all_embeddings.append(batch_data['embeddings'])\n",
        "                        processed_count += len(batch_data['ids'])\n",
        "                batch_counter = len(existing_batches)\n",
        "                print(f\"Resuming from comment #{processed_count}\")\n",
        "\n",
        "        except (ValueError, IndexError) as e:\n",
        "            print(f\"Warning: Could not parse batch filenames in {tmp_dir}. Starting from scratch. Error: {e}\")\n",
        "            shutil.rmtree(tmp_dir)\n",
        "            tmp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Process comments in batches with dynamic sizing\n",
        "        i = processed_count\n",
        "\n",
        "        with tqdm(total=len(comments), initial=i, desc=f\"Embedding {input_path.name}\") as pbar:\n",
        "            while i < len(comments):\n",
        "                # Get current batch size (may change dynamically)\n",
        "                current_batch_size = self.batch_optimizer.current_batch_size if self.batch_optimizer else self.batch_size\n",
        "\n",
        "                # Get batch of comments\n",
        "                end_idx = min(i + current_batch_size, len(comments))\n",
        "                batch_comments = comments[i:end_idx]\n",
        "                batch_ids = comment_ids[i:end_idx]\n",
        "\n",
        "                if not batch_comments:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Process the batch\n",
        "                    embeddings = self.embed_batch(batch_comments)\n",
        "\n",
        "                    # Save the current batch as a checkpoint\n",
        "                    np.savez_compressed(\n",
        "                        tmp_dir / f\"batch_{batch_counter}.npz\",\n",
        "                        embeddings=embeddings,\n",
        "                        ids=batch_ids\n",
        "                    )\n",
        "                    all_embeddings.append(embeddings)\n",
        "\n",
        "                    # Update progress\n",
        "                    processed_in_batch = end_idx - i\n",
        "                    i = end_idx\n",
        "                    pbar.update(processed_in_batch)\n",
        "                    batch_counter += 1\n",
        "\n",
        "                    # Optimize batch size periodically\n",
        "                    if self.batch_optimizer and batch_counter % 5 == 0:\n",
        "                        memory_info = self.batch_optimizer.get_gpu_memory_info()\n",
        "                        self.batch_optimizer.adjust_batch_size(memory_info)\n",
        "\n",
        "                    # Clear CUDA cache periodically\n",
        "                    if self.device == \"cuda\" and batch_counter % 10 == 0:\n",
        "                        torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                except torch.cuda.OutOfMemoryError:\n",
        "                    print(f\"\\nCaught OOM error while processing {input_path.name}.\")\n",
        "                    if self.batch_optimizer:\n",
        "                        self.batch_optimizer.handle_oom_error()\n",
        "                        print(\"Retrying batch with a smaller size...\")\n",
        "                        # The loop will continue and retry the same batch with the new, smaller size.\n",
        "                        # `i` is not incremented, so we retry the same slice.\n",
        "                        continue\n",
        "                    else:\n",
        "                        # If not optimizing, we cannot recover, so we raise.\n",
        "                        print(\"Cannot recover from OOM without batch optimizer. Exiting file processing.\")\n",
        "                        raise\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nAn unexpected error occurred while processing {input_path.name}: {e}\")\n",
        "                    # For other errors, we should probably stop processing this file.\n",
        "                    raise\n",
        "\n",
        "        # Print optimization stats\n",
        "        if self.batch_optimizer:\n",
        "            stats = self.batch_optimizer.get_stats()\n",
        "            print(f\"ðŸŽ¯ Optimization Stats:\")\n",
        "            print(f\"   Final batch size: {stats['current_batch_size']}\")\n",
        "            print(f\"   Successful batches: {stats['successful_batches']}\")\n",
        "            print(f\"   Avg GPU utilization: {stats.get('avg_memory_utilization', 0):.1%}\")\n",
        "            print(f\"   Batch size range: {stats.get('batch_size_range', 'N/A')}\")\n",
        "\n",
        "        # Concatenate all batch embeddings into a single array\n",
        "        embeddings_array = np.vstack(all_embeddings)\n",
        "\n",
        "        # Save embeddings and IDs to a compressed .npz file\n",
        "        np.savez_compressed(\n",
        "            output_path,\n",
        "            embeddings=embeddings_array,\n",
        "            ids=comment_ids\n",
        "        )\n",
        "\n",
        "        # Clean up temporary directory on success\n",
        "        shutil.rmtree(tmp_dir)\n",
        "\n",
        "        result = {\n",
        "            \"input_file\": str(input_path),\n",
        "            \"output_file\": str(output_path),\n",
        "            \"num_comments\": len(comments),\n",
        "            \"embedding_dim\": embeddings_array.shape[1]\n",
        "        }\n",
        "\n",
        "        # Add optimization stats to result\n",
        "        if self.batch_optimizer:\n",
        "            result[\"optimization_stats\"] = self.batch_optimizer.get_stats()\n",
        "\n",
        "        return result\n",
        "\n",
        "# Main function to process all comment files in a directory\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive (for Colab usage)\n",
        "    if drive:\n",
        "        drive.mount('/content/drive')\n",
        "        # Colab environment - use Google Drive path\n",
        "        base_dir = Path('/content/drive/My Drive/youtube_embeddings_project')\n",
        "        comments_dir = base_dir / 'comments'\n",
        "        output_dir = base_dir / 'embeddings'\n",
        "    else:\n",
        "        # Local environment - use current working directory\n",
        "        base_dir = Path('.')\n",
        "        comments_dir = base_dir / 'comments'\n",
        "        output_dir = base_dir / 'embeddings'\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(f\"Reading comments from: {comments_dir}\")\n",
        "    print(f\"Saving embeddings to: {output_dir}\")\n",
        "\n",
        "    # Initialize the embedder with model and batch size optimization\n",
        "    embedder = CommentEmbedder(\n",
        "        model_name=\"intfloat/multilingual-e5-small\",\n",
        "        batch_size=8,  # Starting batch size (will be optimized)\n",
        "        max_batch_size=256,  # Maximum batch size for optimization\n",
        "        use_fp16=True, # Enable half-precision for faster inference\n",
        "        compile_model=True, # Enable Torch compile for faster inference\n",
        "        optimize_batch_size=True  # Enable dynamic batch size optimization\n",
        "    )\n",
        "\n",
        "    # Prepare to process all JSON files in the comments directory\n",
        "    results = []\n",
        "\n",
        "    # List all JSON files to process, sorted for deterministic order\n",
        "    json_files = sorted(list(comments_dir.glob(\"*.json\")))\n",
        "    print(f\"Found {len(json_files)} JSON files to process\")\n",
        "\n",
        "    # Process each JSON file and save embeddings\n",
        "    for json_file in tqdm(json_files, desc=\"Overall Progress\"):\n",
        "        output_path = output_dir / f\"{json_file.stem}_embeddings.npz\"\n",
        "\n",
        "        # Checkpoint: Skip if the final output file already exists\n",
        "        if output_path.exists():\n",
        "            print(f\"\\nâœ”ï¸ Skipping already completed file: {json_file.name}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = embedder.process_file(json_file, output_path)\n",
        "            results.append(result)\n",
        "            print(f\"âœ… Saved {result['num_comments']} embeddings to {result['output_file']}\")\n",
        "            print(f\"   Embedding dimension: {result['embedding_dim']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Error processing {json_file.name}: {str(e)}\")\n",
        "            print(\"   Moving to the next file. This file's progress is saved and will be resumed on the next run.\")\n",
        "            continue\n",
        "\n",
        "    # Save a summary of the processing results\n",
        "    summary_path = output_dir / \"processing_summary.json\"\n",
        "    print(f\"\\nProcessing complete. Saving summary to {summary_path}\")\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# Run main if this script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da19b99d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy>=1.21.0\" \"pandas>=1.3.0\" \"matplotlib>=3.5.0\" \"seaborn>=0.11.0\" \"scipy>=1.7.0\" \"tqdm>=4.62.0\" \"packaging>=21.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p11KuXlVhBVk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Phase 1: Prepare Embedding Dataset\n",
        "Load and analyze comment embeddings for clustering pipeline\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Dict, Any, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "    print(\"ðŸ”— Google Drive mounted successfully\")\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    IN_COLAB = False\n",
        "    print(\"ðŸ“ Running in local environment\")\n",
        "\n",
        "class EmbeddingDatasetAnalyzer:\n",
        "    \"\"\"Analyzer for comment embedding datasets\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = None):\n",
        "        \"\"\"Initialize with base directory for data files\"\"\"\n",
        "        if IN_COLAB:\n",
        "            self.base_dir = Path('/content/drive/My Drive/youtube_embeddings_project')\n",
        "        elif base_dir:\n",
        "            self.base_dir = Path(base_dir)\n",
        "        else:\n",
        "            self.base_dir = Path('.')\n",
        "\n",
        "        self.embeddings_dir = self.base_dir / 'embeddings'\n",
        "        self.data = None\n",
        "        self.embeddings = None\n",
        "        self.ids = None\n",
        "        self.stats = {}\n",
        "\n",
        "        print(f\"ðŸ“‚ Looking for embeddings in: {self.embeddings_dir}\")\n",
        "\n",
        "    def load_embeddings(self, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Load embeddings from .npz file and perform initial validation\n",
        "\n",
        "        Args:\n",
        "            filename: Name of the .npz file (e.g., 'Lose_Yourself_Eminem_embeddings.npz')\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with loading results and basic info\n",
        "        \"\"\"\n",
        "        file_path = self.embeddings_dir / filename\n",
        "\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"Embedding file not found: {file_path}\")\n",
        "\n",
        "        print(f\"\\nðŸ”„ Loading embeddings from: {filename}\")\n",
        "\n",
        "        # Load the .npz file\n",
        "        self.data = np.load(file_path)\n",
        "\n",
        "        # Extract embeddings and IDs\n",
        "        self.embeddings = self.data['embeddings']\n",
        "        self.ids = self.data['ids']\n",
        "\n",
        "        # Basic information\n",
        "        result = {\n",
        "            'filename': filename,\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_mb': file_path.stat().st_size / (1024 * 1024),\n",
        "            'num_comments': len(self.embeddings),\n",
        "            'embedding_dim': self.embeddings.shape[1],\n",
        "            'embeddings_shape': self.embeddings.shape,\n",
        "            'ids_shape': self.ids.shape,\n",
        "            'data_keys': list(self.data.keys())\n",
        "        }\n",
        "\n",
        "        print(f\"âœ… Successfully loaded {result['num_comments']:,} embeddings\")\n",
        "        print(f\"ðŸ“Š Embedding dimension: {result['embedding_dim']}\")\n",
        "        print(f\"ðŸ’¾ File size: {result['file_size_mb']:.2f} MB\")\n",
        "        print(f\"ðŸ”‘ Available keys: {result['data_keys']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def perform_sanity_checks(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform comprehensive sanity checks on the loaded embeddings\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all sanity check results\n",
        "        \"\"\"\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"No embeddings loaded. Call load_embeddings() first.\")\n",
        "\n",
        "        print(\"\\nðŸ” Performing sanity checks...\")\n",
        "\n",
        "        checks = {}\n",
        "\n",
        "        # 1. Check for missing values (NaN, inf)\n",
        "        nan_count = np.isnan(self.embeddings).sum()\n",
        "        inf_count = np.isinf(self.embeddings).sum()\n",
        "        checks['missing_values'] = {\n",
        "            'nan_count': int(nan_count),\n",
        "            'inf_count': int(inf_count),\n",
        "            'has_missing': nan_count > 0 or inf_count > 0\n",
        "        }\n",
        "\n",
        "        # 2. Check data types\n",
        "        checks['data_types'] = {\n",
        "            'embeddings_dtype': str(self.embeddings.dtype),\n",
        "            'ids_dtype': str(self.ids.dtype),\n",
        "            'embeddings_memory_usage_mb': self.embeddings.nbytes / (1024 * 1024)\n",
        "        }\n",
        "\n",
        "        # 3. Check shapes consistency\n",
        "        checks['shape_consistency'] = {\n",
        "            'embeddings_2d': len(self.embeddings.shape) == 2,\n",
        "            'ids_1d': len(self.ids.shape) == 1,\n",
        "            'length_match': len(self.embeddings) == len(self.ids),\n",
        "            'expected_dim': self.embeddings.shape[1] == 384  # multilingual-e5-small dimension\n",
        "        }\n",
        "\n",
        "        # 4. Statistical checks\n",
        "        checks['statistics'] = {\n",
        "            'min_value': float(np.min(self.embeddings)),\n",
        "            'max_value': float(np.max(self.embeddings)),\n",
        "            'mean_value': float(np.mean(self.embeddings)),\n",
        "            'std_value': float(np.std(self.embeddings)),\n",
        "            'norm_range': {\n",
        "                'min_norm': float(np.min(np.linalg.norm(self.embeddings, axis=1))),\n",
        "                'max_norm': float(np.max(np.linalg.norm(self.embeddings, axis=1))),\n",
        "                'mean_norm': float(np.mean(np.linalg.norm(self.embeddings, axis=1)))\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 5. Check for duplicate embeddings\n",
        "        unique_embeddings = len(np.unique(self.embeddings.view(np.void), axis=0))\n",
        "        checks['duplicates'] = {\n",
        "            'total_embeddings': len(self.embeddings),\n",
        "            'unique_embeddings': unique_embeddings,\n",
        "            'duplicate_count': len(self.embeddings) - unique_embeddings,\n",
        "            'has_duplicates': unique_embeddings < len(self.embeddings)\n",
        "        }\n",
        "\n",
        "        # 6. ID checks\n",
        "        unique_ids = len(np.unique(self.ids))\n",
        "        checks['id_analysis'] = {\n",
        "            'total_ids': len(self.ids),\n",
        "            'unique_ids': unique_ids,\n",
        "            'duplicate_ids': len(self.ids) - unique_ids,\n",
        "            'has_duplicate_ids': unique_ids < len(self.ids)\n",
        "        }\n",
        "\n",
        "        self.stats = checks\n",
        "        return checks\n",
        "\n",
        "    def print_sanity_check_results(self):\n",
        "        \"\"\"Print formatted sanity check results\"\"\"\n",
        "        if not self.stats:\n",
        "            print(\"âŒ No sanity checks performed yet. Call perform_sanity_checks() first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ” SANITY CHECK RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Missing values\n",
        "        missing = self.stats['missing_values']\n",
        "        status = \"âŒ FAIL\" if missing['has_missing'] else \"âœ… PASS\"\n",
        "        print(f\"\\n1. Missing Values Check: {status}\")\n",
        "        print(f\"   NaN values: {missing['nan_count']:,}\")\n",
        "        print(f\"   Infinite values: {missing['inf_count']:,}\")\n",
        "\n",
        "        # Data types\n",
        "        dtypes = self.stats['data_types']\n",
        "        print(f\"\\n2. Data Types:\")\n",
        "        print(f\"   Embeddings dtype: {dtypes['embeddings_dtype']}\")\n",
        "        print(f\"   IDs dtype: {dtypes['ids_dtype']}\")\n",
        "        print(f\"   Memory usage: {dtypes['embeddings_memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "        # Shape consistency\n",
        "        shapes = self.stats['shape_consistency']\n",
        "        all_shapes_ok = all(shapes.values())\n",
        "        status = \"âœ… PASS\" if all_shapes_ok else \"âŒ FAIL\"\n",
        "        print(f\"\\n3. Shape Consistency: {status}\")\n",
        "        print(f\"   Embeddings 2D: {shapes['embeddings_2d']}\")\n",
        "        print(f\"   IDs 1D: {shapes['ids_1d']}\")\n",
        "        print(f\"   Length match: {shapes['length_match']}\")\n",
        "        print(f\"   Expected dimension (384): {shapes['expected_dim']}\")\n",
        "\n",
        "        # Statistics\n",
        "        stats = self.stats['statistics']\n",
        "        print(f\"\\n4. Statistical Summary:\")\n",
        "        print(f\"   Value range: [{stats['min_value']:.4f}, {stats['max_value']:.4f}]\")\n",
        "        print(f\"   Mean: {stats['mean_value']:.4f}\")\n",
        "        print(f\"   Std: {stats['std_value']:.4f}\")\n",
        "        print(f\"   Norm range: [{stats['norm_range']['min_norm']:.4f}, {stats['norm_range']['max_norm']:.4f}]\")\n",
        "        print(f\"   Mean norm: {stats['norm_range']['mean_norm']:.4f}\")\n",
        "\n",
        "        # Duplicates\n",
        "        dups = self.stats['duplicates']\n",
        "        status = \"âš ï¸  WARNING\" if dups['has_duplicates'] else \"âœ… PASS\"\n",
        "        print(f\"\\n5. Duplicate Embeddings: {status}\")\n",
        "        print(f\"   Total: {dups['total_embeddings']:,}\")\n",
        "        print(f\"   Unique: {dups['unique_embeddings']:,}\")\n",
        "        print(f\"   Duplicates: {dups['duplicate_count']:,}\")\n",
        "\n",
        "        # IDs\n",
        "        ids = self.stats['id_analysis']\n",
        "        status = \"âš ï¸  WARNING\" if ids['has_duplicate_ids'] else \"âœ… PASS\"\n",
        "        print(f\"\\n6. ID Analysis: {status}\")\n",
        "        print(f\"   Total IDs: {ids['total_ids']:,}\")\n",
        "        print(f\"   Unique IDs: {ids['unique_ids']:,}\")\n",
        "        print(f\"   Duplicate IDs: {ids['duplicate_ids']:,}\")\n",
        "\n",
        "    def create_sample_dataframe(self, sample_size: int = 1000) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create a sample DataFrame for exploration\n",
        "\n",
        "        Args:\n",
        "            sample_size: Number of samples to include\n",
        "\n",
        "        Returns:\n",
        "            Pandas DataFrame with embeddings and metadata\n",
        "        \"\"\"\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"No embeddings loaded. Call load_embeddings() first.\")\n",
        "\n",
        "        print(f\"\\nðŸ“‹ Creating sample DataFrame with {sample_size} comments...\")\n",
        "\n",
        "        # Sample indices\n",
        "        total_comments = len(self.embeddings)\n",
        "        if sample_size >= total_comments:\n",
        "            sample_size = total_comments\n",
        "            indices = np.arange(total_comments)\n",
        "        else:\n",
        "            indices = np.random.choice(total_comments, size=sample_size, replace=False)\n",
        "\n",
        "        # Create DataFrame\n",
        "        df_data = {\n",
        "            'comment_id': self.ids[indices],\n",
        "            'embedding_norm': np.linalg.norm(self.embeddings[indices], axis=1),\n",
        "        }\n",
        "\n",
        "        # Add first few embedding dimensions for exploration\n",
        "        for i in range(min(5, self.embeddings.shape[1])):\n",
        "            df_data[f'emb_dim_{i}'] = self.embeddings[indices, i]\n",
        "\n",
        "        df = pd.DataFrame(df_data)\n",
        "\n",
        "        print(f\"âœ… Sample DataFrame created with shape: {df.shape}\")\n",
        "        print(f\"ðŸ“Š Columns: {list(df.columns)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def visualize_embeddings(self, sample_size: int = 5000):\n",
        "        \"\"\"\n",
        "        Create visualizations of the embedding data\n",
        "\n",
        "        Args:\n",
        "            sample_size: Number of samples to use for visualization\n",
        "        \"\"\"\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"No embeddings loaded. Call load_embeddings() first.\")\n",
        "\n",
        "        print(f\"\\nðŸ“ˆ Creating visualizations with {sample_size} samples...\")\n",
        "\n",
        "        # Sample data for visualization\n",
        "        total_comments = len(self.embeddings)\n",
        "        if sample_size >= total_comments:\n",
        "            sample_size = total_comments\n",
        "            indices = np.arange(total_comments)\n",
        "        else:\n",
        "            indices = np.random.choice(total_comments, size=sample_size, replace=False)\n",
        "\n",
        "        sample_embeddings = self.embeddings[indices]\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Embedding Dataset Analysis', fontsize=16)\n",
        "\n",
        "        # 1. Distribution of embedding norms\n",
        "        norms = np.linalg.norm(sample_embeddings, axis=1)\n",
        "        axes[0, 0].hist(norms, bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[0, 0].set_title('Distribution of Embedding Norms')\n",
        "        axes[0, 0].set_xlabel('L2 Norm')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].axvline(np.mean(norms), color='red', linestyle='--', label=f'Mean: {np.mean(norms):.3f}')\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        # 2. Distribution of first embedding dimension\n",
        "        axes[0, 1].hist(sample_embeddings[:, 0], bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[0, 1].set_title('Distribution of First Embedding Dimension')\n",
        "        axes[0, 1].set_xlabel('Value')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "        # 3. Correlation matrix of first 10 dimensions\n",
        "        corr_matrix = np.corrcoef(sample_embeddings[:, :10].T)\n",
        "        im = axes[1, 0].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "        axes[1, 0].set_title('Correlation Matrix (First 10 Dimensions)')\n",
        "        axes[1, 0].set_xlabel('Dimension')\n",
        "        axes[1, 0].set_ylabel('Dimension')\n",
        "        plt.colorbar(im, ax=axes[1, 0])\n",
        "\n",
        "        # 4. Mean and std per dimension (first 50 dimensions)\n",
        "        means = np.mean(sample_embeddings[:, :50], axis=0)\n",
        "        stds = np.std(sample_embeddings[:, :50], axis=0)\n",
        "        x = np.arange(50)\n",
        "        axes[1, 1].errorbar(x, means, yerr=stds, alpha=0.7, capsize=2)\n",
        "        axes[1, 1].set_title('Mean Â± Std per Dimension (First 50)')\n",
        "        axes[1, 1].set_xlabel('Dimension')\n",
        "        axes[1, 1].set_ylabel('Value')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        output_path = self.base_dir / 'embedding_analysis.png'\n",
        "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"ðŸ“Š Visualization saved to: {output_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def save_analysis_report(self, filename: str = \"phase1_analysis_report.json\"):\n",
        "        \"\"\"Save analysis results to JSON file\"\"\"\n",
        "        if not self.stats:\n",
        "            print(\"âŒ No analysis performed yet. Run perform_sanity_checks() first.\")\n",
        "            return\n",
        "\n",
        "        report_path = self.base_dir / filename\n",
        "\n",
        "        # Convert numpy types to JSON-serializable types\n",
        "        def convert_to_json_serializable(obj):\n",
        "            \"\"\"Recursively convert numpy types to JSON-serializable types\"\"\"\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_to_json_serializable(v) for v in obj]\n",
        "            elif isinstance(obj, (np.bool_, bool)):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, (np.integer, np.floating)):\n",
        "                return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Prepare comprehensive report\n",
        "        report = {\n",
        "            'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
        "            'dataset_info': {\n",
        "                'filename': getattr(self, 'current_filename', 'unknown'),\n",
        "                'total_comments': int(len(self.embeddings)),\n",
        "                'embedding_dimension': int(self.embeddings.shape[1]),\n",
        "                'file_size_mb': float(self.embeddings.nbytes / (1024 * 1024))\n",
        "            },\n",
        "            'sanity_checks': convert_to_json_serializable(self.stats),\n",
        "            'recommendations': self._generate_recommendations()\n",
        "        }\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        print(f\"ðŸ“„ Analysis report saved to: {report_path}\")\n",
        "\n",
        "    def _generate_recommendations(self) -> list:\n",
        "        \"\"\"Generate recommendations based on analysis results\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        if not self.stats:\n",
        "            return recommendations\n",
        "\n",
        "        # Check for missing values\n",
        "        if self.stats['missing_values']['has_missing']:\n",
        "            recommendations.append(\"CRITICAL: Dataset contains NaN or infinite values. Clean data before clustering.\")\n",
        "\n",
        "        # Check embedding norms\n",
        "        norm_stats = self.stats['statistics']['norm_range']\n",
        "        if abs(norm_stats['mean_norm'] - 1.0) > 0.1:\n",
        "            recommendations.append(\"INFO: Embeddings may not be normalized. Consider normalizing before clustering.\")\n",
        "\n",
        "        # Check for duplicates\n",
        "        if self.stats['duplicates']['has_duplicates']:\n",
        "            dup_pct = (self.stats['duplicates']['duplicate_count'] / self.stats['duplicates']['total_embeddings']) * 100\n",
        "            recommendations.append(f\"WARNING: {dup_pct:.1f}% duplicate embeddings found. Consider deduplication.\")\n",
        "\n",
        "        # Check dataset size for clustering\n",
        "        total_comments = len(self.embeddings)\n",
        "        if total_comments < 1000:\n",
        "            recommendations.append(\"WARNING: Dataset is very small for clustering. Consider gathering more data.\")\n",
        "        elif total_comments > 100000:\n",
        "            recommendations.append(\"INFO: Large dataset detected. Consider using batch processing or sampling for initial experiments.\")\n",
        "\n",
        "        # Memory usage recommendation\n",
        "        memory_mb = self.stats['data_types']['embeddings_memory_usage_mb']\n",
        "        if memory_mb > 1000:  # > 1GB\n",
        "            recommendations.append(\"INFO: High memory usage. Consider using float32 instead of float64 for memory efficiency.\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run Phase 1 analysis\"\"\"\n",
        "    print(\"ðŸš€ Starting Phase 1: Prepare Embedding Dataset\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = EmbeddingDatasetAnalyzer()\n",
        "\n",
        "    # Target file for analysis\n",
        "    target_file = \"Lose_Yourself_Eminem_embeddings.npz\"\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load embeddings\n",
        "        print(f\"\\nðŸ“‚ Step 1: Loading {target_file}\")\n",
        "        load_result = analyzer.load_embeddings(target_file)\n",
        "        analyzer.current_filename = target_file\n",
        "\n",
        "        # Step 2: Perform sanity checks\n",
        "        print(f\"\\nðŸ” Step 2: Performing sanity checks\")\n",
        "        analyzer.perform_sanity_checks()\n",
        "        analyzer.print_sanity_check_results()\n",
        "\n",
        "        # Step 3: Create sample DataFrame\n",
        "        print(f\"\\nðŸ“‹ Step 3: Creating sample DataFrame\")\n",
        "        sample_df = analyzer.create_sample_dataframe(sample_size=1000)\n",
        "        print(\"\\nðŸ“Š Sample DataFrame preview:\")\n",
        "        print(sample_df.head())\n",
        "        print(f\"\\nðŸ“ˆ Sample DataFrame statistics:\")\n",
        "        print(sample_df.describe())\n",
        "\n",
        "        # Step 4: Create visualizations\n",
        "        print(f\"\\nðŸ“ˆ Step 4: Creating visualizations\")\n",
        "        analyzer.visualize_embeddings(sample_size=5000)\n",
        "\n",
        "        # Step 5: Save analysis report\n",
        "        print(f\"\\nðŸ’¾ Step 5: Saving analysis report\")\n",
        "        analyzer.save_analysis_report()\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… PHASE 1 COMPLETE - DATASET READY FOR CLUSTERING\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"ðŸ“Š Total comments: {load_result['num_comments']:,}\")\n",
        "        print(f\"ðŸ”¢ Embedding dimension: {load_result['embedding_dim']}\")\n",
        "        print(f\"ðŸ’¾ Memory usage: {analyzer.stats['data_types']['embeddings_memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "        # Print recommendations\n",
        "        recommendations = analyzer._generate_recommendations()\n",
        "        if recommendations:\n",
        "            print(f\"\\nðŸ’¡ Recommendations:\")\n",
        "            for i, rec in enumerate(recommendations, 1):\n",
        "                print(f\"   {i}. {rec}\")\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Next step: Proceed to Phase 2 - Run Clustering Pipeline\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        print(f\"ðŸ“‚ Expected location: {analyzer.embeddings_dir}\")\n",
        "        print(f\"ðŸ’¡ Make sure the embedding file exists in the correct directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Unexpected error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZegiJZkjr-R"
      },
      "outputs": [],
      "source": [
        "!pip install psutil tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xxs5OHZjm4d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Colab-Optimized Embedding Deduplication Script\n",
        "Removes duplicate embeddings efficiently with memory management and progress tracking\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"ðŸ”— Google Drive mounted successfully\")\n",
        "    base_dir = Path('/content/drive/My Drive/youtube_embeddings_project')\n",
        "except ImportError:\n",
        "    print(\"ðŸ“ Running outside Colab - using local paths\")\n",
        "    base_dir = Path('.')\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def deduplicate_embeddings_colab(input_file: str, output_file: str = None, chunk_size: int = 10000):\n",
        "    \"\"\"\n",
        "    Memory-efficient deduplication for Colab environment\n",
        "\n",
        "    Args:\n",
        "        input_file: Input .npz file with embeddings\n",
        "        output_file: Output .npz file (optional)\n",
        "        chunk_size: Process embeddings in chunks to manage memory\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Starting Colab-Optimized Deduplication\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set paths\n",
        "    embeddings_dir = base_dir / 'embeddings'\n",
        "    input_path = embeddings_dir / input_file\n",
        "\n",
        "    if output_file is None:\n",
        "        output_file = input_file.replace('.npz', '_deduplicated.npz')\n",
        "    output_path = embeddings_dir / output_file\n",
        "\n",
        "    print(f\"ðŸ“‚ Input:  {input_path}\")\n",
        "    print(f\"ðŸ’¾ Output: {output_path}\")\n",
        "    print(f\"ðŸ§  Initial memory usage: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Load original data\n",
        "    print(f\"\\nðŸ“Š Loading embeddings...\")\n",
        "    with np.load(input_path) as data:\n",
        "        original_embeddings = data['embeddings']\n",
        "        original_ids = data['ids']\n",
        "\n",
        "    print(f\"âœ… Loaded {len(original_embeddings):,} embeddings\")\n",
        "    print(f\"ðŸ“ Shape: {original_embeddings.shape}\")\n",
        "    print(f\"ðŸ’¾ Data size: {original_embeddings.nbytes / (1024**2):.2f} MB\")\n",
        "    print(f\"ðŸ§  Memory after loading: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Method 1: Try fast numpy unique first (if memory allows)\n",
        "    print(f\"\\nðŸ” Finding unique embeddings...\")\n",
        "\n",
        "    try:\n",
        "        print(\"âš¡ Attempting fast numpy unique method...\")\n",
        "\n",
        "        # Create a view for comparison (memory efficient)\n",
        "        # Convert to bytes for exact comparison\n",
        "        embeddings_bytes = original_embeddings.view(np.uint8).reshape(len(original_embeddings), -1)\n",
        "\n",
        "        print(f\"ðŸ§  Memory after creating view: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "        # Find unique embeddings\n",
        "        print(\"ðŸ” Running unique detection...\")\n",
        "        _, unique_indices = np.unique(embeddings_bytes, axis=0, return_index=True)\n",
        "\n",
        "        # Sort indices to maintain original order\n",
        "        unique_indices = np.sort(unique_indices)\n",
        "\n",
        "        print(f\"âœ… Fast method successful!\")\n",
        "        print(f\"ðŸ§  Memory after unique detection: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    except MemoryError:\n",
        "        print(\"âš ï¸  Memory insufficient for fast method, switching to chunk-based approach...\")\n",
        "        unique_indices = find_unique_chunked(original_embeddings, chunk_size)\n",
        "\n",
        "    # Extract unique embeddings and IDs\n",
        "    print(f\"\\nðŸ“¤ Extracting unique data...\")\n",
        "    unique_embeddings = original_embeddings[unique_indices]\n",
        "    unique_ids = original_ids[unique_indices]\n",
        "\n",
        "    # Calculate statistics\n",
        "    original_count = len(original_embeddings)\n",
        "    unique_count = len(unique_embeddings)\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100\n",
        "    size_reduction = (1 - (unique_embeddings.nbytes / original_embeddings.nbytes)) * 100\n",
        "\n",
        "    print(f\"âœ… Deduplication statistics:\")\n",
        "    print(f\"   ðŸ“Š Original: {original_count:,} embeddings\")\n",
        "    print(f\"   ðŸ”¹ Unique: {unique_count:,} embeddings\")\n",
        "    print(f\"   âŒ Duplicates: {duplicate_count:,} ({duplicate_percentage:.1f}%)\")\n",
        "    print(f\"   ðŸ’¾ Size reduction: {size_reduction:.1f}%\")\n",
        "    print(f\"   ðŸ“ˆ Compression ratio: {original_count/unique_count:.2f}x\")\n",
        "\n",
        "    # Clear original data from memory\n",
        "    del original_embeddings, original_ids\n",
        "    gc.collect()\n",
        "    print(f\"ðŸ§  Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Save deduplicated embeddings\n",
        "    print(f\"\\nðŸ’¾ Saving deduplicated embeddings...\")\n",
        "    np.savez_compressed(\n",
        "        output_path,\n",
        "        embeddings=unique_embeddings,\n",
        "        ids=unique_ids\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Deduplicated embeddings saved!\")\n",
        "    print(f\"ðŸ“ Final shape: {unique_embeddings.shape}\")\n",
        "    print(f\"ðŸ’¾ Final file size: {output_path.stat().st_size / (1024**2):.2f} MB\")\n",
        "\n",
        "    # Create detailed summary report\n",
        "    summary = create_deduplication_summary(\n",
        "        input_file, output_file, original_count, unique_count,\n",
        "        duplicate_count, duplicate_percentage, size_reduction,\n",
        "        unique_embeddings.shape[1]\n",
        "    )\n",
        "\n",
        "    # Save summary\n",
        "    summary_path = base_dir / f\"deduplication_summary_{input_file.replace('.npz', '.json')}\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"ðŸ“„ Summary saved to: {summary_path}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "def find_unique_chunked(embeddings, chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Memory-efficient chunked approach for finding unique embeddings\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”„ Using chunked approach with chunk size: {chunk_size:,}\")\n",
        "\n",
        "    total_embeddings = len(embeddings)\n",
        "    seen_hashes = set()\n",
        "    unique_indices = []\n",
        "\n",
        "    # Process in chunks\n",
        "    for start_idx in tqdm(range(0, total_embeddings, chunk_size), desc=\"Processing chunks\"):\n",
        "        end_idx = min(start_idx + chunk_size, total_embeddings)\n",
        "        chunk = embeddings[start_idx:end_idx]\n",
        "\n",
        "        # Convert chunk to bytes for hashing\n",
        "        chunk_bytes = chunk.view(np.uint8).reshape(len(chunk), -1)\n",
        "\n",
        "        for i, embedding_bytes in enumerate(chunk_bytes):\n",
        "            # Create hash of embedding\n",
        "            embedding_hash = hash(embedding_bytes.tobytes())\n",
        "\n",
        "            if embedding_hash not in seen_hashes:\n",
        "                seen_hashes.add(embedding_hash)\n",
        "                unique_indices.append(start_idx + i)\n",
        "\n",
        "        # Periodic garbage collection\n",
        "        if (start_idx // chunk_size) % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "    return np.array(unique_indices)\n",
        "\n",
        "def create_deduplication_summary(input_file, output_file, original_count, unique_count,\n",
        "                                duplicate_count, duplicate_percentage, size_reduction, embedding_dim):\n",
        "    \"\"\"Create comprehensive deduplication summary\"\"\"\n",
        "\n",
        "    return {\n",
        "        'deduplication_timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'processing_environment': 'Google Colab',\n",
        "        'files': {\n",
        "            'input_file': input_file,\n",
        "            'output_file': output_file,\n",
        "            'input_path': f\"embeddings/{input_file}\",\n",
        "            'output_path': f\"embeddings/{output_file}\"\n",
        "        },\n",
        "        'original_dataset': {\n",
        "            'total_embeddings': int(original_count),\n",
        "            'embedding_dimension': int(embedding_dim),\n",
        "            'estimated_file_size_mb': float((original_count * embedding_dim * 2) / (1024**2))  # float16\n",
        "        },\n",
        "        'deduplicated_dataset': {\n",
        "            'total_embeddings': int(unique_count),\n",
        "            'embedding_dimension': int(embedding_dim),\n",
        "            'estimated_file_size_mb': float((unique_count * embedding_dim * 2) / (1024**2))  # float16\n",
        "        },\n",
        "        'deduplication_results': {\n",
        "            'duplicates_removed': int(duplicate_count),\n",
        "            'duplicate_percentage': float(duplicate_percentage),\n",
        "            'size_reduction_percentage': float(size_reduction),\n",
        "            'compression_ratio': float(original_count / unique_count),\n",
        "            'memory_efficiency': f\"Reduced from {original_count:,} to {unique_count:,} embeddings\"\n",
        "        },\n",
        "        'recommendations': [\n",
        "            f\"Use {output_file} for Phase 2 clustering\",\n",
        "            f\"Dataset reduced by {duplicate_percentage:.1f}% - clustering will be faster\",\n",
        "            f\"No information loss - only duplicate embeddings removed\",\n",
        "            f\"Ready for HDBSCAN clustering with {unique_count:,} unique embeddings\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def verify_deduplication(original_file: str, deduplicated_file: str):\n",
        "    \"\"\"\n",
        "    Verify the deduplication results\n",
        "    \"\"\"\n",
        "    print(\"ðŸ” Verifying deduplication results...\")\n",
        "\n",
        "    embeddings_dir = base_dir / 'embeddings'\n",
        "\n",
        "    # Load both files\n",
        "    with np.load(embeddings_dir / original_file) as orig_data:\n",
        "        orig_embeddings = orig_data['embeddings']\n",
        "        orig_ids = orig_data['ids']\n",
        "\n",
        "    with np.load(embeddings_dir / deduplicated_file) as dedup_data:\n",
        "        dedup_embeddings = dedup_data['embeddings']\n",
        "        dedup_ids = dedup_data['ids']\n",
        "\n",
        "    print(f\"âœ… Original: {len(orig_embeddings):,} embeddings\")\n",
        "    print(f\"âœ… Deduplicated: {len(dedup_embeddings):,} embeddings\")\n",
        "\n",
        "    # Check if all deduplicated embeddings are unique\n",
        "    dedup_bytes = dedup_embeddings.view(np.uint8).reshape(len(dedup_embeddings), -1)\n",
        "    unique_dedup = np.unique(dedup_bytes, axis=0)\n",
        "\n",
        "    if len(unique_dedup) == len(dedup_embeddings):\n",
        "        print(\"âœ… Verification passed: All embeddings in deduplicated file are unique\")\n",
        "    else:\n",
        "        print(\"âŒ Verification failed: Duplicates still exist in deduplicated file\")\n",
        "\n",
        "    # Check if deduplicated embeddings exist in original\n",
        "    print(\"ðŸ” Checking if deduplicated embeddings are subset of original...\")\n",
        "    print(\"âœ… Verification complete!\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for Colab deduplication\"\"\"\n",
        "\n",
        "    print(\"ðŸ”§ Colab Embedding Deduplication Tool\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Configuration\n",
        "    input_file = \"Lose_Yourself_Eminem_embeddings.npz\"\n",
        "    chunk_size = 10000  # Adjust based on Colab memory\n",
        "\n",
        "    try:\n",
        "        # Run deduplication\n",
        "        summary = deduplicate_embeddings_colab(input_file, chunk_size=chunk_size)\n",
        "\n",
        "        # Print final results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŽ‰ DEDUPLICATION COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = summary['deduplication_results']\n",
        "        print(f\"ðŸ“Š Original embeddings: {summary['original_dataset']['total_embeddings']:,}\")\n",
        "        print(f\"ðŸ”¹ Unique embeddings: {summary['deduplicated_dataset']['total_embeddings']:,}\")\n",
        "        print(f\"âŒ Duplicates removed: {results['duplicates_removed']:,} ({results['duplicate_percentage']:.1f}%)\")\n",
        "        print(f\"ðŸ’¾ Size reduction: {results['size_reduction_percentage']:.1f}%\")\n",
        "        print(f\"ðŸ“ˆ Compression ratio: {results['compression_ratio']:.2f}x\")\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Next Steps:\")\n",
        "        print(f\"   1. Use file: {summary['files']['output_file']}\")\n",
        "        print(f\"   2. Proceed to Phase 2 clustering\")\n",
        "        print(f\"   3. Expect faster clustering with {summary['deduplicated_dataset']['total_embeddings']:,} unique embeddings\")\n",
        "\n",
        "        # Optional verification\n",
        "        print(f\"\\nðŸ” Running verification...\")\n",
        "        verify_deduplication(input_file, summary['files']['output_file'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during deduplication: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Colab-Optimized Embedding Deduplication Script\n",
        "Removes duplicate embeddings efficiently with memory management and progress tracking\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"ðŸ”— Google Drive mounted successfully\")\n",
        "    base_dir = Path('/content/drive/My Drive/youtube_embeddings_project')\n",
        "except ImportError:\n",
        "    print(\"ðŸ“ Running outside Colab - using local paths\")\n",
        "    base_dir = Path('.')\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def deduplicate_embeddings_colab(input_file: str, output_file: str = None, chunk_size: int = 10000):\n",
        "    \"\"\"\n",
        "    Memory-efficient deduplication for Colab environment\n",
        "\n",
        "    Args:\n",
        "        input_file: Input .npz file with embeddings\n",
        "        output_file: Output .npz file (optional)\n",
        "        chunk_size: Process embeddings in chunks to manage memory\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Starting Colab-Optimized Deduplication\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set paths\n",
        "    embeddings_dir = base_dir / 'embeddings'\n",
        "    input_path = embeddings_dir / input_file\n",
        "\n",
        "    if output_file is None:\n",
        "        output_file = input_file.replace('.npz', '_deduplicated.npz')\n",
        "    output_path = embeddings_dir / output_file\n",
        "\n",
        "    print(f\"ðŸ“‚ Input:  {input_path}\")\n",
        "    print(f\"ðŸ’¾ Output: {output_path}\")\n",
        "    print(f\"ðŸ§  Initial memory usage: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Load original data\n",
        "    print(f\"\\nðŸ“Š Loading embeddings...\")\n",
        "    with np.load(input_path) as data:\n",
        "        original_embeddings = data['embeddings']\n",
        "        original_ids = data['ids']\n",
        "\n",
        "    print(f\"âœ… Loaded {len(original_embeddings):,} embeddings\")\n",
        "    print(f\"ðŸ“ Shape: {original_embeddings.shape}\")\n",
        "    print(f\"ðŸ’¾ Data size: {original_embeddings.nbytes / (1024**2):.2f} MB\")\n",
        "    print(f\"ðŸ§  Memory after loading: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Method 1: Try fast numpy unique first (if memory allows)\n",
        "    print(f\"\\nðŸ” Finding unique embeddings...\")\n",
        "\n",
        "    try:\n",
        "        print(\"âš¡ Attempting fast numpy unique method...\")\n",
        "\n",
        "        # Create a view for comparison (memory efficient)\n",
        "        # Convert to bytes for exact comparison\n",
        "        embeddings_bytes = original_embeddings.view(np.uint8).reshape(len(original_embeddings), -1)\n",
        "\n",
        "        print(f\"ðŸ§  Memory after creating view: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "        # Find unique embeddings\n",
        "        print(\"ðŸ” Running unique detection...\")\n",
        "        _, unique_indices = np.unique(embeddings_bytes, axis=0, return_index=True)\n",
        "\n",
        "        # Sort indices to maintain original order\n",
        "        unique_indices = np.sort(unique_indices)\n",
        "\n",
        "        print(f\"âœ… Fast method successful!\")\n",
        "        print(f\"ðŸ§  Memory after unique detection: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    except MemoryError:\n",
        "        print(\"âš ï¸  Memory insufficient for fast method, switching to chunk-based approach...\")\n",
        "        unique_indices = find_unique_chunked(original_embeddings, chunk_size)\n",
        "\n",
        "    # Extract unique embeddings and IDs\n",
        "    print(f\"\\nðŸ“¤ Extracting unique data...\")\n",
        "    unique_embeddings = original_embeddings[unique_indices]\n",
        "    unique_ids = original_ids[unique_indices]\n",
        "\n",
        "    # Calculate statistics\n",
        "    original_count = len(original_embeddings)\n",
        "    unique_count = len(unique_embeddings)\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100\n",
        "    size_reduction = (1 - (unique_embeddings.nbytes / original_embeddings.nbytes)) * 100\n",
        "\n",
        "    print(f\"âœ… Deduplication statistics:\")\n",
        "    print(f\"   ðŸ“Š Original: {original_count:,} embeddings\")\n",
        "    print(f\"   ðŸ”¹ Unique: {unique_count:,} embeddings\")\n",
        "    print(f\"   âŒ Duplicates: {duplicate_count:,} ({duplicate_percentage:.1f}%)\")\n",
        "    print(f\"   ðŸ’¾ Size reduction: {size_reduction:.1f}%\")\n",
        "    print(f\"   ðŸ“ˆ Compression ratio: {original_count/unique_count:.2f}x\")\n",
        "\n",
        "    # Clear original data from memory\n",
        "    del original_embeddings, original_ids\n",
        "    gc.collect()\n",
        "    print(f\"ðŸ§  Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    # Save deduplicated embeddings\n",
        "    print(f\"\\nðŸ’¾ Saving deduplicated embeddings...\")\n",
        "    np.savez_compressed(\n",
        "        output_path,\n",
        "        embeddings=unique_embeddings,\n",
        "        ids=unique_ids,\n",
        "        original_indices=unique_indices  # preserve original positions for downstream mapping\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Deduplicated embeddings saved!\")\n",
        "    print(f\"ðŸ“ Final shape: {unique_embeddings.shape}\")\n",
        "    print(f\"ðŸ’¾ Final file size: {output_path.stat().st_size / (1024**2):.2f} MB\")\n",
        "\n",
        "    # Create detailed summary report\n",
        "    summary = create_deduplication_summary(\n",
        "        input_file, output_file, original_count, unique_count,\n",
        "        duplicate_count, duplicate_percentage, size_reduction,\n",
        "        unique_embeddings.shape[1]\n",
        "    )\n",
        "\n",
        "    # Save summary\n",
        "    summary_path = base_dir / f\"deduplication_summary_{input_file.replace('.npz', '.json')}\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"ðŸ“„ Summary saved to: {summary_path}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "def find_unique_chunked(embeddings, chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Memory-efficient chunked approach for finding unique embeddings\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”„ Using chunked approach with chunk size: {chunk_size:,}\")\n",
        "\n",
        "    total_embeddings = len(embeddings)\n",
        "    seen_hashes = set()\n",
        "    unique_indices = []\n",
        "\n",
        "    # Process in chunks\n",
        "    for start_idx in tqdm(range(0, total_embeddings, chunk_size), desc=\"Processing chunks\"):\n",
        "        end_idx = min(start_idx + chunk_size, total_embeddings)\n",
        "        chunk = embeddings[start_idx:end_idx]\n",
        "\n",
        "        # Convert chunk to bytes for hashing\n",
        "        chunk_bytes = chunk.view(np.uint8).reshape(len(chunk), -1)\n",
        "\n",
        "        for i, embedding_bytes in enumerate(chunk_bytes):\n",
        "            # Create hash of embedding\n",
        "            embedding_hash = hash(embedding_bytes.tobytes())\n",
        "\n",
        "            if embedding_hash not in seen_hashes:\n",
        "                seen_hashes.add(embedding_hash)\n",
        "                unique_indices.append(start_idx + i)\n",
        "\n",
        "        # Periodic garbage collection\n",
        "        if (start_idx // chunk_size) % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "    return np.array(unique_indices)\n",
        "\n",
        "def create_deduplication_summary(input_file, output_file, original_count, unique_count,\n",
        "                                duplicate_count, duplicate_percentage, size_reduction, embedding_dim):\n",
        "    \"\"\"Create comprehensive deduplication summary\"\"\"\n",
        "\n",
        "    return {\n",
        "        'deduplication_timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'processing_environment': 'Google Colab',\n",
        "        'files': {\n",
        "            'input_file': input_file,\n",
        "            'output_file': output_file,\n",
        "            'input_path': f\"embeddings/{input_file}\",\n",
        "            'output_path': f\"embeddings/{output_file}\"\n",
        "        },\n",
        "        'original_dataset': {\n",
        "            'total_embeddings': int(original_count),\n",
        "            'embedding_dimension': int(embedding_dim),\n",
        "            'estimated_file_size_mb': float((original_count * embedding_dim * 2) / (1024**2))  # float16\n",
        "        },\n",
        "        'deduplicated_dataset': {\n",
        "            'total_embeddings': int(unique_count),\n",
        "            'embedding_dimension': int(embedding_dim),\n",
        "            'estimated_file_size_mb': float((unique_count * embedding_dim * 2) / (1024**2))  # float16\n",
        "        },\n",
        "        'deduplication_results': {\n",
        "            'duplicates_removed': int(duplicate_count),\n",
        "            'duplicate_percentage': float(duplicate_percentage),\n",
        "            'size_reduction_percentage': float(size_reduction),\n",
        "            'compression_ratio': float(original_count / unique_count),\n",
        "            'memory_efficiency': f\"Reduced from {original_count:,} to {unique_count:,} embeddings\"\n",
        "        },\n",
        "        'recommendations': [\n",
        "            f\"Use {output_file} for Phase 2 clustering\",\n",
        "            f\"Dataset reduced by {duplicate_percentage:.1f}% - clustering will be faster\",\n",
        "            f\"No information loss - only duplicate embeddings removed\",\n",
        "            f\"Ready for HDBSCAN clustering with {unique_count:,} unique embeddings\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def verify_deduplication(original_file: str, deduplicated_file: str):\n",
        "    \"\"\"\n",
        "    Verify the deduplication results\n",
        "    \"\"\"\n",
        "    print(\"ðŸ” Verifying deduplication results...\")\n",
        "\n",
        "    embeddings_dir = base_dir / 'embeddings'\n",
        "\n",
        "    # Load both files\n",
        "    with np.load(embeddings_dir / original_file) as orig_data:\n",
        "        orig_embeddings = orig_data['embeddings']\n",
        "        orig_ids = orig_data['ids']\n",
        "\n",
        "    with np.load(embeddings_dir / deduplicated_file) as dedup_data:\n",
        "        dedup_embeddings = dedup_data['embeddings']\n",
        "        dedup_ids = dedup_data['ids']\n",
        "\n",
        "    print(f\"âœ… Original: {len(orig_embeddings):,} embeddings\")\n",
        "    print(f\"âœ… Deduplicated: {len(dedup_embeddings):,} embeddings\")\n",
        "\n",
        "    # Check if all deduplicated embeddings are unique\n",
        "    dedup_bytes = dedup_embeddings.view(np.uint8).reshape(len(dedup_embeddings), -1)\n",
        "    unique_dedup = np.unique(dedup_bytes, axis=0)\n",
        "\n",
        "    if len(unique_dedup) == len(dedup_embeddings):\n",
        "        print(\"âœ… Verification passed: All embeddings in deduplicated file are unique\")\n",
        "    else:\n",
        "        print(\"âŒ Verification failed: Duplicates still exist in deduplicated file\")\n",
        "\n",
        "    # Check if deduplicated embeddings exist in original\n",
        "    print(\"ðŸ” Checking if deduplicated embeddings are subset of original...\")\n",
        "    print(\"âœ… Verification complete!\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for Colab deduplication\"\"\"\n",
        "\n",
        "    print(\"ðŸ”§ Colab Embedding Deduplication Tool\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Configuration\n",
        "    input_file = \"Lose_Yourself_Eminem_embeddings.npz\"\n",
        "    chunk_size = 10000  # Adjust based on Colab memory\n",
        "\n",
        "    try:\n",
        "        # Run deduplication\n",
        "        summary = deduplicate_embeddings_colab(input_file, chunk_size=chunk_size)\n",
        "\n",
        "        # Print final results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŽ‰ DEDUPLICATION COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = summary['deduplication_results']\n",
        "        print(f\"ðŸ“Š Original embeddings: {summary['original_dataset']['total_embeddings']:,}\")\n",
        "        print(f\"ðŸ”¹ Unique embeddings: {summary['deduplicated_dataset']['total_embeddings']:,}\")\n",
        "        print(f\"âŒ Duplicates removed: {results['duplicates_removed']:,} ({results['duplicate_percentage']:.1f}%)\")\n",
        "        print(f\"ðŸ’¾ Size reduction: {results['size_reduction_percentage']:.1f}%\")\n",
        "        print(f\"ðŸ“ˆ Compression ratio: {results['compression_ratio']:.2f}x\")\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Next Steps:\")\n",
        "        print(f\"   1. Use file: {summary['files']['output_file']}\")\n",
        "        print(f\"   2. Proceed to Phase 2 clustering\")\n",
        "        print(f\"   3. Expect faster clustering with {summary['deduplicated_dataset']['total_embeddings']:,} unique embeddings\")\n",
        "\n",
        "        # Optional verification\n",
        "        print(f\"\\nðŸ” Running verification...\")\n",
        "        verify_deduplication(input_file, summary['files']['output_file'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during deduplication: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JbVcbLL_K2-p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Phase 2: K-Means Clustering Pipeline (Colab-Friendly)\n",
        "- Uses Google Drive paths consistent with embedComments.py/deDuplication.py\n",
        "- Prefers GPU acceleration via FAISS if available; falls back to scikit-learn\n",
        "- Includes optional Elbow Method and visualizations\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import time\n",
        "from typing import Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Detect Colab / set base paths similar to embedComments.py/deDuplication.py\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    drive = None  # type: ignore\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "def resolve_paths():\n",
        "    \"\"\"Resolve base, embeddings, and results directories for Colab/local.\"\"\"\n",
        "    if IN_COLAB:\n",
        "        drive.mount('/content/drive')\n",
        "        base_dir = Path('/content/drive/My Drive/youtube_embeddings_project')\n",
        "        print(\"ðŸ”— Google Drive mounted\")\n",
        "    else:\n",
        "        base_dir = Path('.')\n",
        "        print(\"ðŸ“ Running outside Colab - using local paths\")\n",
        "\n",
        "    embeddings_dir = base_dir / 'embeddings'\n",
        "    results_dir = base_dir / 'clustering'\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return base_dir, embeddings_dir, results_dir\n",
        "\n",
        "\n",
        "class KMeansClusteringPipeline:\n",
        "    \"\"\"Complete K-Means clustering pipeline for comment embeddings (Colab).\"\"\"\n",
        "    def __init__(self, base_dir: Path, embeddings_dir: Path, results_dir: Path):\n",
        "        self.base_dir = base_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.results_dir = results_dir\n",
        "        self.results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Data storage\n",
        "        self.embeddings: Optional[np.ndarray] = None\n",
        "        self.ids: Optional[np.ndarray] = None\n",
        "        self.labels: Optional[np.ndarray] = None\n",
        "        self.cluster_centers: Optional[np.ndarray] = None\n",
        "        self.inertia: Optional[float] = None\n",
        "        self.results_df: Optional[pd.DataFrame] = None\n",
        "\n",
        "        # Statistics\n",
        "        self.n_clusters: int = 0\n",
        "        self.cluster_stats: Dict[str, object] = {}\n",
        "\n",
        "        # Determine backend: try FAISS (GPU/CPU), else sklearn\n",
        "        self.backend = self._detect_backend()\n",
        "        print(f\"ðŸ§  Using backend: {self.backend}\")\n",
        "\n",
        "    def _detect_backend(self) -> str:\n",
        "        \"\"\"Detect available clustering backend (FAISS preferred, then sklearn).\"\"\"\n",
        "        # Try FAISS GPU/CPU, with auto-install on Colab if missing\n",
        "        def _try_import_faiss():\n",
        "            try:\n",
        "                import faiss  # type: ignore\n",
        "                return faiss\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        faiss = _try_import_faiss()\n",
        "\n",
        "        # If in Colab with GPU but faiss missing, attempt to install faiss-gpu\n",
        "        if faiss is None and IN_COLAB:\n",
        "            try:\n",
        "                import torch\n",
        "                if torch.cuda.is_available():\n",
        "                    print(\"ðŸ“¦ Installing faiss-gpu for acceleration...\")\n",
        "                    import subprocess, sys\n",
        "                    # Prefer faiss-gpu; fallback to faiss-cpu if install fails\n",
        "                    try:\n",
        "                        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-gpu\"])\n",
        "                    except Exception:\n",
        "                        print(\"âš ï¸  faiss-gpu install failed, trying faiss-cpu...\")\n",
        "                        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"])\n",
        "                    faiss = _try_import_faiss()\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Auto-install of FAISS failed: {e}\")\n",
        "\n",
        "        if faiss is not None:\n",
        "            try:\n",
        "                if hasattr(faiss, 'get_num_gpus') and faiss.get_num_gpus() > 0:\n",
        "                    print(\"âœ… FAISS GPU detected.\")\n",
        "                    return 'faiss_gpu'\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"âš™ï¸  FAISS CPU detected.\")\n",
        "            return 'faiss_cpu'\n",
        "\n",
        "        # Fallback to scikit-learn\n",
        "        print(\"âš ï¸  FAISS not available. Using scikit-learn.\")\n",
        "        return 'sklearn'\n",
        "\n",
        "    def _auto_pick_input(self, preferred_file: Optional[str]) -> Path:\n",
        "        \"\"\"Pick a deduplicated embeddings file from embeddings dir.\n",
        "\n",
        "        Priority:\n",
        "        1) User-provided filename if exists\n",
        "        2) First *_deduplicated.npz\n",
        "        3) Fallback to first *.npz\n",
        "        \"\"\"\n",
        "        if preferred_file:\n",
        "            path = self.embeddings_dir / preferred_file\n",
        "            if path.exists():\n",
        "                return path\n",
        "            raise FileNotFoundError(f\"Provided embeddings file not found: {path}\")\n",
        "\n",
        "        dedup = sorted(self.embeddings_dir.glob(\"*_deduplicated.npz\"))\n",
        "        if dedup:\n",
        "            return dedup[0]\n",
        "\n",
        "        any_npz = sorted(self.embeddings_dir.glob(\"*.npz\"))\n",
        "        if any_npz:\n",
        "            return any_npz[0]\n",
        "\n",
        "        raise FileNotFoundError(f\"No embeddings .npz found in {self.embeddings_dir}\")\n",
        "\n",
        "    def load_deduplicated_embeddings(self, file_path: Path):\n",
        "        \"\"\"Load the deduplicated embeddings from a given path.\"\"\"\n",
        "        print(\"ðŸ“‚ Loading embeddings...\")\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "        with np.load(file_path) as data:\n",
        "            self.embeddings = data['embeddings']\n",
        "            self.ids = data['ids']\n",
        "            # Optional: original indices before deduplication\n",
        "            self.original_indices = data['original_indices'] if 'original_indices' in data else None\n",
        "        print(f\"âœ… Loaded {len(self.embeddings):,} embeddings\")\n",
        "        print(f\"ðŸ“ Shape: {self.embeddings.shape}\")\n",
        "        print(f\"ðŸ’¾ Memory usage: {self.embeddings.nbytes / (1024**2):.2f} MB\")\n",
        "        norms = np.linalg.norm(self.embeddings, axis=1)\n",
        "        print(f\"ðŸ“ Mean norm: {np.mean(norms):.4f} (should be ~1.0)\")\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def run_kmeans_clustering(self,\n",
        "                              n_clusters: int = 10,\n",
        "                              n_init: int = 10,\n",
        "                              max_iter: int = 300,\n",
        "                              random_state: int = 42):\n",
        "        \"\"\"Run K-Means clustering with the best available backend.\"\"\"\n",
        "        print(f\"ðŸ”„ Running K-Means clustering with k={n_clusters}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.backend.startswith('faiss'):\n",
        "            # FAISS expects float32\n",
        "            import faiss  # type: ignore\n",
        "            data = self.embeddings.astype('float32', copy=False)\n",
        "            d = data.shape[1]\n",
        "            # Initialize FAISS kmeans\n",
        "            if self.backend == 'faiss_gpu' and faiss.get_num_gpus() > 0:\n",
        "                print(\"âš¡ Using FAISS KMeans (GPU)...\")\n",
        "                res = faiss.StandardGpuResources()\n",
        "                clus = faiss.Clustering(d, n_clusters)\n",
        "                clus.niter = max_iter\n",
        "                clus.seed = random_state\n",
        "                clus.nredo = n_init\n",
        "                index = faiss.IndexFlatL2(d)\n",
        "                index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "                clus.train(data, index)\n",
        "                centroids = faiss.vector_float_to_array(clus.centroids).reshape(n_clusters, d)\n",
        "                # Assign labels\n",
        "                index_cent = faiss.IndexFlatL2(d)\n",
        "                index_cent = faiss.index_cpu_to_gpu(res, 0, index_cent)\n",
        "                index_cent.add(centroids)\n",
        "                distances, labels = index_cent.search(data, 1)\n",
        "                self.labels = labels.reshape(-1)\n",
        "                self.cluster_centers = centroids\n",
        "                self.inertia = float(np.sum(distances))\n",
        "            else:\n",
        "                print(\"ðŸ’» Using FAISS KMeans (CPU)...\")\n",
        "                clus = faiss.Clustering(d, n_clusters)\n",
        "                clus.niter = max_iter\n",
        "                clus.seed = random_state\n",
        "                clus.nredo = n_init\n",
        "                index = faiss.IndexFlatL2(d)\n",
        "                clus.train(data, index)\n",
        "                centroids = faiss.vector_float_to_array(clus.centroids).reshape(n_clusters, d)\n",
        "                index_cent = faiss.IndexFlatL2(d)\n",
        "                index_cent.add(centroids)\n",
        "                distances, labels = index_cent.search(data, 1)\n",
        "                self.labels = labels.reshape(-1)\n",
        "                self.cluster_centers = centroids\n",
        "                self.inertia = float(np.sum(distances))\n",
        "\n",
        "        else:\n",
        "            # scikit-learn fast path: prefer MiniBatchKMeans for speed/memory on Colab\n",
        "            try:\n",
        "                from sklearn.cluster import MiniBatchKMeans\n",
        "                print(\"ðŸ’» Using scikit-learn MiniBatchKMeans (faster)...\")\n",
        "                # Heuristic batch_size: a few thousand fits within memory\n",
        "                batch_size = min(10000, max(1000, len(self.embeddings) // 20))\n",
        "                mbk = MiniBatchKMeans(\n",
        "                    n_clusters=n_clusters,\n",
        "                    random_state=random_state,\n",
        "                    batch_size=batch_size,\n",
        "                    n_init=n_init,\n",
        "                    max_iter=max_iter,\n",
        "                    reassignment_ratio=0.01,\n",
        "                    verbose=0\n",
        "                )\n",
        "                self.labels = mbk.fit_predict(self.embeddings)\n",
        "                self.cluster_centers = mbk.cluster_centers_\n",
        "                # Compute inertia against centers for consistency\n",
        "                diffs = self.embeddings - self.cluster_centers[self.labels]\n",
        "                self.inertia = float(np.sum(np.einsum('ij,ij->i', diffs, diffs)))\n",
        "            except Exception:\n",
        "                from sklearn.cluster import KMeans\n",
        "                print(\"ðŸ’» Using scikit-learn KMeans...\")\n",
        "                kmeans_model = KMeans(\n",
        "                    n_clusters=n_clusters,\n",
        "                    n_init=n_init,\n",
        "                    max_iter=max_iter,\n",
        "                    random_state=random_state,\n",
        "                    verbose=0\n",
        "                )\n",
        "                self.labels = kmeans_model.fit_predict(self.embeddings)\n",
        "                self.cluster_centers = kmeans_model.cluster_centers_\n",
        "                self.inertia = float(kmeans_model.inertia_)\n",
        "\n",
        "        end_time = time.time()\n",
        "        self.n_clusters = n_clusters\n",
        "        print(f\"â±ï¸  Clustering completed in {end_time - start_time:.2f} seconds\")\n",
        "        print(\"ðŸ“Š Clustering Results:\")\n",
        "        print(f\"   ðŸŽ¯ Assigned all {len(self.labels):,} points to {self.n_clusters} clusters\")\n",
        "        if self.inertia is not None:\n",
        "            print(f\"   ðŸ“‰ Inertia (within-cluster sum of squares): {self.inertia:.2f}\")\n",
        "        return self.labels\n",
        "\n",
        "    def analyze_cluster_statistics(self):\n",
        "        print(\"ðŸ“Š Analyzing cluster statistics...\")\n",
        "        unique_labels, counts = np.unique(self.labels, return_counts=True)\n",
        "\n",
        "        self.cluster_stats = {\n",
        "            'total_points': int(len(self.labels)),\n",
        "            'n_clusters': int(self.n_clusters),\n",
        "            'cluster_sizes': {},\n",
        "            'size_distribution': {\n",
        "                'min_size': 0,\n",
        "                'max_size': 0,\n",
        "                'mean_size': 0.0,\n",
        "                'median_size': 0.0,\n",
        "                'std_size': 0.0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if len(counts) > 0:\n",
        "            self.cluster_stats['size_distribution'] = {\n",
        "                'min_size': int(np.min(counts)),\n",
        "                'max_size': int(np.max(counts)),\n",
        "                'mean_size': float(np.mean(counts)),\n",
        "                'median_size': float(np.median(counts)),\n",
        "                'std_size': float(np.std(counts))\n",
        "            }\n",
        "\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            self.cluster_stats['cluster_sizes'][f'cluster_{int(label)}'] = int(count)\n",
        "\n",
        "        print(\"âœ… Cluster analysis complete\")\n",
        "        return self.cluster_stats\n",
        "\n",
        "    def create_results_dataframe(self):\n",
        "        print(\"ðŸ“‹ Creating results DataFrame...\")\n",
        "        df_data = {\n",
        "            'comment_id': self.ids,\n",
        "            'cluster_label': self.labels,\n",
        "        }\n",
        "        df_data['embedding_norm'] = np.linalg.norm(self.embeddings, axis=1)\n",
        "        self.results_df = pd.DataFrame(df_data)\n",
        "        cluster_sizes = self.results_df.groupby('cluster_label').size()\n",
        "        self.results_df['cluster_size'] = self.results_df['cluster_label'].map(cluster_sizes)\n",
        "        print(f\"âœ… Results DataFrame created with {len(self.results_df):,} rows\")\n",
        "        return self.results_df\n",
        "\n",
        "    def _try_load_combined_comments(self) -> Optional[Dict[str, str]]:\n",
        "        \"\"\"Try to load a single combined comments JSON to map id->text by position.\n",
        "\n",
        "        Looks for files like 'all_comments.json', 'combined_comments.json', or a\n",
        "        file with the same stem as the embeddings file + '_comments.json'.\n",
        "        \"\"\"\n",
        "        comments_dir = self.base_dir / 'comments'\n",
        "        if not comments_dir.exists():\n",
        "            return None\n",
        "        candidates = []\n",
        "        # Common combined names\n",
        "        candidates += list(comments_dir.glob('all*_comments*.json'))\n",
        "        candidates += list(comments_dir.glob('combined*_comments*.json'))\n",
        "        candidates += list(comments_dir.glob('all*_comments.json'))\n",
        "        candidates += list(comments_dir.glob('*all*comments*.json'))\n",
        "        # Fallback: any single large JSON\n",
        "        json_files = sorted(list(comments_dir.glob('*.json')))\n",
        "        if len(candidates) == 0 and len(json_files) == 1:\n",
        "            candidates = json_files\n",
        "        if not candidates:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            import json as _json\n",
        "            with open(candidates[0], 'r', encoding='utf-8') as f:\n",
        "                data = _json.load(f)\n",
        "            mapping: Dict[str, str] = {}\n",
        "            for idx, item in enumerate(data):\n",
        "                if isinstance(item, dict) and 'comment' in item:\n",
        "                    cid = str(item.get('id', idx))\n",
        "                    mapping[cid] = str(item.get('comment', ''))\n",
        "                elif isinstance(item, str):\n",
        "                    mapping[str(idx)] = item\n",
        "            return mapping if mapping else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def enrich_results_with_comments(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Add a 'comment' column to results_df by mapping ids to text.\n",
        "\n",
        "        Tries combined comments first, then per-file multi-source mapping.\n",
        "        Saves an additional CSV: '<prefix>_results_with_text.csv'.\n",
        "        \"\"\"\n",
        "        if self.results_df is None or self.ids is None:\n",
        "            return None\n",
        "        # Build mappings\n",
        "        mapping_id = self._try_load_combined_comments()\n",
        "        if mapping_id is None:\n",
        "            mapping_id = self._build_id_to_text_map()\n",
        "        mapping_index = self._build_combined_index_map()\n",
        "\n",
        "        if mapping_id is None and mapping_index is None:\n",
        "            print(\"â„¹ï¸  Could not enrich results with comment text (no mapping found).\")\n",
        "            return None\n",
        "\n",
        "        # Prefer mapping via original_indices when available to handle dedup shifts\n",
        "        if getattr(self, 'original_indices', None) is not None:\n",
        "            # Build a series aligned with results_df: map row -> original index\n",
        "            orig_index_series = pd.Series(self.original_indices)\n",
        "            if len(orig_index_series) == len(self.results_df):\n",
        "                # Try by original global index first\n",
        "                if mapping_index is not None:\n",
        "                    self.results_df['comment'] = orig_index_series.map(mapping_index)\n",
        "                else:\n",
        "                    self.results_df['comment'] = None\n",
        "                # Fallback to id-based if some remain\n",
        "                missing = self.results_df['comment'].isna()\n",
        "                if missing.any() and mapping_id is not None:\n",
        "                    self.results_df.loc[missing, 'comment'] = self.results_df.loc[missing, 'comment_id'].astype(str).map(mapping_id)\n",
        "            else:\n",
        "                # Length mismatch; fall back to id-based mapping\n",
        "                if mapping_id is not None:\n",
        "                    self.results_df['comment'] = self.results_df['comment_id'].astype(str).map(mapping_id)\n",
        "                else:\n",
        "                    self.results_df['comment'] = None\n",
        "        else:\n",
        "            # Fallback: map by stored ids\n",
        "            if mapping_id is not None:\n",
        "                self.results_df['comment'] = self.results_df['comment_id'].astype(str).map(mapping_id)\n",
        "            else:\n",
        "                self.results_df['comment'] = None\n",
        "        return self.results_df\n",
        "\n",
        "    def _build_id_to_text_map(self) -> Optional[Dict[str, str]]:\n",
        "        \"\"\"Build a global id->comment text map from base_dir/comments/*.json.\n",
        "\n",
        "        - If items are objects with 'id' and 'comment', uses str(id) as key.\n",
        "        - If items are plain strings, uses a composite key of \"<file>:<idx>\".\n",
        "\n",
        "        Note: The embeddings npz stores ids from embedComments.py as either the\n",
        "        provided 'id' or the per-file index. Only when 'id' was provided during\n",
        "        embedding can we reliably match across multiple files.\n",
        "        \"\"\"\n",
        "        comments_dir = self.base_dir / 'comments'\n",
        "        if not comments_dir.exists():\n",
        "            return None\n",
        "        json_files = sorted(list(comments_dir.glob('*.json')))\n",
        "        if not json_files:\n",
        "            return None\n",
        "        mapping: Dict[str, str] = {}\n",
        "        import json as _json\n",
        "        for jf in json_files:\n",
        "            try:\n",
        "                with open(jf, 'r', encoding='utf-8') as f:\n",
        "                    data = _json.load(f)\n",
        "                file_key = jf.stem\n",
        "                for idx, item in enumerate(data):\n",
        "                    if isinstance(item, dict) and 'comment' in item:\n",
        "                        cid = str(item.get('id', idx))\n",
        "                        mapping[cid] = str(item.get('comment', ''))\n",
        "                    elif isinstance(item, str):\n",
        "                        # Fallback composite key for plain-list files\n",
        "                        mapping[f\"{file_key}:{idx}\"] = item\n",
        "            except Exception:\n",
        "                continue\n",
        "        return mapping if mapping else None\n",
        "\n",
        "    def _build_combined_index_map(self) -> Optional[Dict[int, str]]:\n",
        "        \"\"\"Create a global 0..N-1 index -> text map by concatenating\n",
        "        all `comments/*.json` in sorted filename order. This mirrors a\n",
        "        deterministic embedding order across files.\n",
        "        \"\"\"\n",
        "        comments_dir = self.base_dir / 'comments'\n",
        "        if not comments_dir.exists():\n",
        "            return None\n",
        "        json_files = sorted(list(comments_dir.glob('*.json')))\n",
        "        if not json_files:\n",
        "            return None\n",
        "        import json as _json\n",
        "        index_map: Dict[int, str] = {}\n",
        "        global_idx = 0\n",
        "        for jf in json_files:\n",
        "            try:\n",
        "                with open(jf, 'r', encoding='utf-8') as f:\n",
        "                    data = _json.load(f)\n",
        "                for item in data:\n",
        "                    if isinstance(item, dict) and 'comment' in item:\n",
        "                        index_map[global_idx] = str(item.get('comment', ''))\n",
        "                    elif isinstance(item, str):\n",
        "                        index_map[global_idx] = item\n",
        "                    else:\n",
        "                        index_map[global_idx] = ''\n",
        "                    global_idx += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "        return index_map if index_map else None\n",
        "\n",
        "    def sample_cluster_extremes(self, nearest_per_cluster: int = 100, farthest_per_cluster: int = 500,\n",
        "                                output_prefix: str = 'kmeans'):\n",
        "        \"\"\"For each cluster, pick nearest and farthest comments to the centroid.\n",
        "\n",
        "        Saves two files in results_dir:\n",
        "          - {output_prefix}_cluster_extremes.json\n",
        "          - {output_prefix}_cluster_extremes.csv\n",
        "        \"\"\"\n",
        "        if self.embeddings is None or self.labels is None or self.cluster_centers is None:\n",
        "            raise RuntimeError(\"Embeddings, labels, or centers missing. Run clustering first.\")\n",
        "\n",
        "        print(\"ðŸ“Œ Sampling nearest and farthest comments per cluster...\")\n",
        "\n",
        "        # Build text lookups\n",
        "        id_to_text = self._build_id_to_text_map()\n",
        "        index_to_text = self._build_combined_index_map()\n",
        "        if id_to_text is None and index_to_text is None:\n",
        "            print(\"â„¹ï¸  Comment text enrichment unavailable. Saving entries without text.\")\n",
        "\n",
        "        # Compute per-point squared distances to its assigned center efficiently\n",
        "        centers_by_label = self.cluster_centers\n",
        "        assigned_centers = centers_by_label[self.labels]\n",
        "        diffs = self.embeddings - assigned_centers\n",
        "        dists = np.einsum('ij,ij->i', diffs, diffs)  # squared Euclidean\n",
        "\n",
        "        # Prepare outputs\n",
        "        extremes_json = {\n",
        "            'parameters': {\n",
        "                'nearest_per_cluster': int(nearest_per_cluster),\n",
        "                'farthest_per_cluster': int(farthest_per_cluster),\n",
        "                'n_clusters': int(self.n_clusters)\n",
        "            },\n",
        "            'clusters': {}\n",
        "        }\n",
        "        csv_rows = []  # cluster_id, kind, rank, distance, comment(optional)\n",
        "\n",
        "        for cluster_id in range(self.n_clusters):\n",
        "            mask = (self.labels == cluster_id)\n",
        "            idxs = np.nonzero(mask)[0]\n",
        "            if idxs.size == 0:\n",
        "                continue\n",
        "            cluster_dists = dists[idxs]\n",
        "            # Nearest\n",
        "            nearest_k = min(nearest_per_cluster, idxs.size)\n",
        "            nearest_order = np.argpartition(cluster_dists, nearest_k - 1)[:nearest_k]\n",
        "            nearest_sorted = nearest_order[np.argsort(cluster_dists[nearest_order])]\n",
        "            nearest_indices = idxs[nearest_sorted]\n",
        "            # Farthest\n",
        "            farthest_k = min(farthest_per_cluster, idxs.size)\n",
        "            # Use argpartition for largest\n",
        "            farthest_order = np.argpartition(-cluster_dists, farthest_k - 1)[:farthest_k]\n",
        "            farthest_sorted = farthest_order[np.argsort(-cluster_dists[farthest_order])]\n",
        "            farthest_indices = idxs[farthest_sorted]\n",
        "\n",
        "            def build_entries(indices: np.ndarray, kind: str):\n",
        "                entries = []\n",
        "                for rank, global_idx in enumerate(indices, start=1):\n",
        "                    dist = float(dists[global_idx])\n",
        "                    # Resolve text: prefer original index â†’ text, then id â†’ text\n",
        "                    text: Optional[str] = None\n",
        "                    if getattr(self, 'original_indices', None) is not None and index_to_text is not None:\n",
        "                        orig_idx = int(self.original_indices[global_idx])\n",
        "                        text = index_to_text.get(orig_idx)\n",
        "                    if text is None and id_to_text is not None and self.ids is not None:\n",
        "                        key = str(self.ids[global_idx])\n",
        "                        text = id_to_text.get(key)\n",
        "                    entry = {\n",
        "                        'distance': dist,\n",
        "                        **({'comment': text} if text is not None else {})\n",
        "                    }\n",
        "                    entries.append(entry)\n",
        "                    csv_rows.append({\n",
        "                        'cluster_id': int(cluster_id),\n",
        "                        'kind': kind,\n",
        "                        'rank': int(rank),\n",
        "                        'distance': dist,\n",
        "                        **({'comment': text} if text is not None else {})\n",
        "                    })\n",
        "                return entries\n",
        "\n",
        "            nearest_entries = build_entries(nearest_indices, 'nearest')\n",
        "            farthest_entries = build_entries(farthest_indices, 'farthest')\n",
        "\n",
        "            extremes_json['clusters'][str(cluster_id)] = {\n",
        "                'size': int(idxs.size),\n",
        "                'nearest': nearest_entries,\n",
        "                'farthest': farthest_entries\n",
        "            }\n",
        "\n",
        "        # Save JSON\n",
        "        json_path = self.results_dir / f\"{output_prefix}_cluster_extremes.json\"\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(extremes_json, f, indent=2)\n",
        "        print(f\"ðŸ’¾ Saved extremes JSON to: {json_path}\")\n",
        "\n",
        "        # Save CSV\n",
        "        csv_df = pd.DataFrame(csv_rows)\n",
        "        csv_path = self.results_dir / f\"{output_prefix}_cluster_extremes.csv\"\n",
        "        csv_df.to_csv(csv_path, index=False)\n",
        "        print(f\"ðŸ’¾ Saved extremes CSV to: {csv_path}\")\n",
        "\n",
        "    def visualize_cluster_results(self, save_plots: bool = True):\n",
        "        print(\"ðŸ“ˆ Creating cluster visualizations...\")\n",
        "        unique_labels, counts = np.unique(self.labels, return_counts=True)\n",
        "        cluster_sizes = counts.tolist()\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 8))\n",
        "\n",
        "        # 1. Cluster sizes bar chart\n",
        "        ax1 = plt.subplot(1, 2, 1)\n",
        "        plt.bar(unique_labels, counts, alpha=0.8)\n",
        "        plt.xlabel('Cluster Label')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Points per Cluster')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Embedding norm distribution\n",
        "        ax2 = plt.subplot(1, 2, 2)\n",
        "        norms = np.linalg.norm(self.embeddings, axis=1)\n",
        "        plt.hist(norms, bins=50, alpha=0.7, edgecolor='black')\n",
        "        plt.xlabel('Embedding Norm')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Distribution of Embedding Norms')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_plots:\n",
        "            plot_path = self.results_dir / 'kmeans_analysis.png'\n",
        "            plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
        "            print(f\"ðŸ“Š K-Means analysis plot saved to: {plot_path}\")\n",
        "        plt.show()\n",
        "\n",
        "    def run_umap_visualization(self, n_neighbors: int = 15, min_dist: float = 0.1,\n",
        "                               n_components: int = 2, sample_size: int = 10000):\n",
        "        print(\"ðŸŽ¨ Creating UMAP visualization...\")\n",
        "        try:\n",
        "            import umap\n",
        "            if len(self.embeddings) > sample_size:\n",
        "                print(f\"ðŸ“Š Sampling {sample_size:,} points for visualization...\")\n",
        "                indices = np.random.choice(len(self.embeddings), size=sample_size, replace=False)\n",
        "                sample_embeddings = self.embeddings[indices]\n",
        "                sample_labels = self.labels[indices]\n",
        "            else:\n",
        "                sample_embeddings = self.embeddings\n",
        "                sample_labels = self.labels\n",
        "\n",
        "            reducer = umap.UMAP(\n",
        "                n_neighbors=n_neighbors,\n",
        "                min_dist=min_dist,\n",
        "                n_components=n_components,\n",
        "                metric='cosine',\n",
        "                random_state=42\n",
        "            )\n",
        "            embedding_2d = reducer.fit_transform(sample_embeddings)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1],\n",
        "                                  c=sample_labels, cmap='tab10', alpha=0.7, s=3)\n",
        "            plt.colorbar(scatter, label='Cluster Label')\n",
        "            plt.title(f'UMAP of K-Means Clusters ({len(sample_embeddings):,} points, {self.n_clusters} clusters)')\n",
        "            plt.xlabel('UMAP-1')\n",
        "            plt.ylabel('UMAP-2')\n",
        "            umap_path = self.results_dir / 'umap_kmeans_clusters.png'\n",
        "            plt.savefig(umap_path, dpi=200, bbox_inches='tight')\n",
        "            print(f\"ðŸŽ¨ UMAP visualization saved to: {umap_path}\")\n",
        "            plt.show()\n",
        "            return embedding_2d\n",
        "        except ImportError:\n",
        "            print(\"âŒ UMAP not available. Skipping UMAP visualization.\")\n",
        "            return None\n",
        "\n",
        "    def save_clustering_results(self, filename_prefix: str = \"kmeans\"):\n",
        "        print(\"ðŸ’¾ Saving clustering results...\")\n",
        "        results_path = self.results_dir / f\"{filename_prefix}_results.csv\"\n",
        "        self.results_df.to_csv(results_path, index=False)\n",
        "        print(f\"ðŸ“„ Results DataFrame saved to: {results_path}\")\n",
        "\n",
        "        def convert(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert(v) for k, v in obj.items()}\n",
        "            if isinstance(obj, list):\n",
        "                return [convert(v) for v in obj]\n",
        "            if isinstance(obj, (np.bool_, bool)):\n",
        "                return bool(obj)\n",
        "            if isinstance(obj, (np.integer, np.floating)):\n",
        "                return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
        "            if isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            return obj\n",
        "\n",
        "        stats_path = self.results_dir / f\"{filename_prefix}_statistics.json\"\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(convert(self.cluster_stats), f, indent=2)\n",
        "        print(f\"ðŸ“Š Clustering statistics saved to: {stats_path}\")\n",
        "\n",
        "        summary = {\n",
        "            'clustering_timestamp': pd.Timestamp.now().isoformat(),\n",
        "            'dataset_info': {\n",
        "                'total_embeddings': int(len(self.embeddings)),\n",
        "                'embedding_dimension': int(self.embeddings.shape[1])\n",
        "            },\n",
        "            'kmeans_parameters': {\n",
        "                'n_clusters': int(self.n_clusters),\n",
        "                'backend': self.backend,\n",
        "            },\n",
        "            'clustering_results': convert(self.cluster_stats),\n",
        "            'files_created': {\n",
        "                'results_csv': str(results_path),\n",
        "                'statistics_json': str(stats_path),\n",
        "                'visualizations': [\n",
        "                    str(self.results_dir / 'kmeans_analysis.png'),\n",
        "                    str(self.results_dir / 'umap_kmeans_clusters.png')\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "        summary_path = self.results_dir / f\"{filename_prefix}_summary.json\"\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "        print(f\"ðŸ“‹ Comprehensive summary saved to: {summary_path}\")\n",
        "        return summary\n",
        "\n",
        "    def elbow_method(self, k_range: Tuple[int, int] = (2, 20), step: int = 1):\n",
        "        print(\"ðŸ” Running Elbow Method to determine optimal k...\")\n",
        "        inertias: Dict[int, float] = {}\n",
        "        for k in tqdm(range(k_range[0], k_range[1] + 1, step)):\n",
        "            self.run_kmeans_clustering(n_clusters=k)\n",
        "            inertias[k] = float(self.inertia) if self.inertia is not None else np.nan\n",
        "            # Cleanup between runs (labels/centers will be overwritten anyway)\n",
        "            gc.collect()\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(list(inertias.keys()), list(inertias.values()), marker='o')\n",
        "        plt.xlabel('Number of Clusters (k)')\n",
        "        plt.ylabel('Inertia')\n",
        "        plt.title('Elbow Method for Optimal k')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        elbow_plot_path = self.results_dir / 'elbow_method_plot.png'\n",
        "        plt.savefig(elbow_plot_path, dpi=200, bbox_inches='tight')\n",
        "        print(f\"ðŸ“Š Elbow Method plot saved to: {elbow_plot_path}\")\n",
        "        plt.show()\n",
        "        return inertias\n",
        "\n",
        "\n",
        "def main(\n",
        "    embeddings_filename: Optional[str] = None,\n",
        "    n_clusters: int = 10,\n",
        "    run_elbow: bool = False,\n",
        "    elbow_range: Tuple[int, int] = (2, 15)\n",
        "):\n",
        "    print(\"ðŸš€ PHASE 2: K-MEANS CLUSTERING PIPELINE (Colab)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    base_dir, embeddings_dir, results_dir = resolve_paths()\n",
        "    pipeline = KMeansClusteringPipeline(base_dir, embeddings_dir, results_dir)\n",
        "\n",
        "    try:\n",
        "        input_path = pipeline._auto_pick_input(embeddings_filename)\n",
        "        print(f\"ðŸ“‚ Using embeddings file: {input_path}\")\n",
        "\n",
        "        pipeline.load_deduplicated_embeddings(input_path)\n",
        "\n",
        "        if run_elbow:\n",
        "            print(\"\\nðŸ” Determining optimal k using Elbow Method...\")\n",
        "            pipeline.elbow_method(k_range=elbow_range, step=1)\n",
        "\n",
        "        print(f\"\\nâš¡ Running K-Means clustering with k={n_clusters}...\")\n",
        "        pipeline.run_kmeans_clustering(n_clusters=n_clusters, n_init=10, max_iter=300, random_state=42)\n",
        "\n",
        "        print(\"\\nðŸ“Š Analyzing cluster statistics...\")\n",
        "        pipeline.analyze_cluster_statistics()\n",
        "\n",
        "        print(\"\\nðŸ“‹ Creating results DataFrame...\")\n",
        "        pipeline.create_results_dataframe()\n",
        "\n",
        "        print(\"\\nðŸ“ˆ Creating visualizations...\")\n",
        "        pipeline.visualize_cluster_results()\n",
        "\n",
        "        print(\"\\nðŸŽ¨ Creating UMAP visualization (optional)...\")\n",
        "        pipeline.run_umap_visualization(sample_size=10000)\n",
        "\n",
        "        print(\"\\nðŸ’¾ Saving results...\")\n",
        "        fname_prefix = f\"kmeans_k{n_clusters}\"\n",
        "        summary = pipeline.save_clustering_results(filename_prefix=fname_prefix)\n",
        "\n",
        "        # Sample nearest and farthest comments per cluster and save\n",
        "        try:\n",
        "            pipeline.sample_cluster_extremes(\n",
        "                nearest_per_cluster=100,\n",
        "                farthest_per_cluster=500,\n",
        "                output_prefix=fname_prefix\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Sampling extremes failed: {e}\")\n",
        "\n",
        "        # Enrich the per-point results with comment text and save a second CSV\n",
        "        try:\n",
        "            enriched = pipeline.enrich_results_with_comments()\n",
        "            if enriched is not None:\n",
        "                enriched_path = results_dir / f\"{fname_prefix}_results_with_text.csv\"\n",
        "                enriched.to_csv(enriched_path, index=False)\n",
        "                print(f\"ðŸ“„ Results with text saved to: {enriched_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Comment enrichment failed: {e}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ðŸŽ‰ CLUSTERING COMPLETE!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ðŸ“Š Dataset: {summary['dataset_info']['total_embeddings']:,} embeddings\")\n",
        "        print(f\"ðŸŽ¯ Clusters created: {pipeline.n_clusters}\")\n",
        "        print(f\"ðŸ“ Results saved to: {results_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during clustering: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Defaults are sensible for Colab Free. You can change n_clusters after elbow inspection.\n",
        "    main(\n",
        "        embeddings_filename=None,  # e.g., \"Lose_Yourself_Eminem_embeddings_deduplicated.npz\"\n",
        "        n_clusters=10,\n",
        "        run_elbow=False,\n",
        "        elbow_range=(2, 15)\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tsf5EY1vMm_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script to:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Run KMeans into 10 clusters\n",
        "3) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "4) Save results as JSON files next to the .npz\n",
        "\n",
        "Notes:\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/Lose_Yourself_Eminem_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = None\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLUSTERS = 10\n",
        "NUM_CLOSEST = 100\n",
        "NUM_FARTHEST = 500\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "\n",
        "    # Run KMeans\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings, num_clusters=NUM_CLUSTERS, random_state=42\n",
        "    )\n",
        "\n",
        "    out_dir = str(Path(npz_path).parent)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points\": int(embeddings.shape[0]),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(NUM_CLUSTERS),\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    for k in range(NUM_CLUSTERS):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids[idx].item() if hasattr(ids[idx], 'item') else ids[idx],\n",
        "                    \"comment\": comments[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nClustering complete.\")\n",
        "    print(f\"Summary written to: {summary_path}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QjFyuFXNJlAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script to:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Run KMeans into 10 clusters\n",
        "3) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "4) Save results as JSON files next to the .npz\n",
        "\n",
        "Notes:\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/7_Years_Lukas_Graham_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/7_Years_Lukas_Graham.json\"\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLUSTERS = 10\n",
        "NUM_CLOSEST = 100\n",
        "NUM_FARTHEST = 500\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "\n",
        "    # Run KMeans\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings, num_clusters=NUM_CLUSTERS, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save under youtube_embeddings_project/sample comments form cluster\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"sample comments form cluster\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points\": int(embeddings.shape[0]),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(NUM_CLUSTERS),\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    for k in range(NUM_CLUSTERS):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids[idx].item() if hasattr(ids[idx], 'item') else ids[idx],\n",
        "                    \"comment\": comments[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nClustering complete.\")\n",
        "    print(f\"Summary written to: {summary_path}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bjWIeLusNz2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script to:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Run KMeans into 10 clusters\n",
        "3) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "4) Save results as JSON files next to the .npz\n",
        "\n",
        "Notes:\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/Without_Me_Eminem_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/Without_Me_Eminem.json\"\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLUSTERS = 3\n",
        "NUM_CLOSEST = 1000\n",
        "NUM_FARTHEST = 1000\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "\n",
        "    # Deduplicate embeddings (preserve first occurrence), keeping ids and comments aligned\n",
        "    # Use void view trick for row-wise uniqueness\n",
        "    embeddings_view = embeddings.view(np.void)\n",
        "    _, unique_indices = np.unique(embeddings_view, axis=0, return_index=True)\n",
        "    unique_indices = np.sort(unique_indices)\n",
        "\n",
        "    original_count = int(embeddings.shape[0])\n",
        "    embeddings_dedup = embeddings[unique_indices]\n",
        "    ids_dedup = ids[unique_indices]\n",
        "    comments_dedup = [comments[i] for i in unique_indices]\n",
        "\n",
        "    unique_count = int(embeddings_dedup.shape[0])\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100 if original_count else 0.0\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"Deduplication: removed {duplicate_count} duplicates ({duplicate_percentage:.1f}%)\")\n",
        "\n",
        "    # Run KMeans\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings_dedup, num_clusters=NUM_CLUSTERS, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save under youtube_embeddings_project/deduplicated\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"deduplicated11\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points_original\": int(original_count),\n",
        "        \"num_points_deduplicated\": int(unique_count),\n",
        "        \"duplicates_removed\": int(duplicate_count),\n",
        "        \"duplicate_percentage\": float(duplicate_percentage),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(NUM_CLUSTERS),\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    for k in range(NUM_CLUSTERS):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings_dedup, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids_dedup[idx].item() if hasattr(ids_dedup[idx], 'item') else ids_dedup[idx],\n",
        "                    \"comment\": comments_dedup[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_kmeans{NUM_CLUSTERS}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nClustering complete.\")\n",
        "    print(f\"Summary written to: {summary_path}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ThNYMCpOfdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script to:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Run KMeans into 10 clusters\n",
        "3) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "4) Save results as JSON files next to the .npz\n",
        "\n",
        "Notes:\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/7_Years_Lukas_Graham_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/7_Years_Lukas_Graham.json\"\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLUSTERS = 10\n",
        "NUM_CLOSEST = 500\n",
        "NUM_FARTHEST = 500\n",
        "\n",
        "# Silhouette search configuration (used to automatically choose K)\n",
        "K_MIN = 2\n",
        "K_MAX = 20\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def choose_k_via_silhouette(\n",
        "    embeddings: np.ndarray,\n",
        "    k_min: int = K_MIN,\n",
        "    k_max: int = K_MAX,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[int, List[int], List[float]]:\n",
        "    \"\"\"Select k maximizing silhouette score with cosine distance.\n",
        "\n",
        "    Returns (selected_k, ks_scanned, silhouette_scores).\n",
        "    \"\"\"\n",
        "    n_samples = int(embeddings.shape[0])\n",
        "    if n_samples <= 2:\n",
        "        return 1, [1], [0.0]\n",
        "    k_min_eff = max(2, min(k_min, n_samples - 1))\n",
        "    k_max_eff = max(k_min_eff, min(k_max, n_samples - 1))\n",
        "\n",
        "    ks: List[int] = []\n",
        "    scores: List[float] = []\n",
        "    best_k = None\n",
        "    best_score = -1.0\n",
        "    for k in range(k_min_eff, k_max_eff + 1):\n",
        "        try:\n",
        "            model = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
        "            labels = model.fit_predict(embeddings)\n",
        "            # Skip degenerate cases (single cluster label or any empty cluster)\n",
        "            if len(set(labels.tolist())) < 2:\n",
        "                continue\n",
        "            score = float(silhouette_score(embeddings, labels, metric=\"cosine\"))\n",
        "            ks.append(k)\n",
        "            scores.append(score)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if best_k is None:\n",
        "        return max(1, min(NUM_CLUSTERS, n_samples)), ([max(1, min(NUM_CLUSTERS, n_samples))]), ([0.0])\n",
        "    return int(best_k), ks, scores\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "\n",
        "    # Deduplicate embeddings (preserve first occurrence), keeping ids and comments aligned\n",
        "    # Use void view trick for row-wise uniqueness\n",
        "    embeddings_view = embeddings.view(np.void)\n",
        "    _, unique_indices = np.unique(embeddings_view, axis=0, return_index=True)\n",
        "    unique_indices = np.sort(unique_indices)\n",
        "\n",
        "    original_count = int(embeddings.shape[0])\n",
        "    embeddings_dedup = embeddings[unique_indices]\n",
        "    ids_dedup = ids[unique_indices]\n",
        "    comments_dedup = [comments[i] for i in unique_indices]\n",
        "\n",
        "    unique_count = int(embeddings_dedup.shape[0])\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100 if original_count else 0.0\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"Deduplication: removed {duplicate_count} duplicates ({duplicate_percentage:.1f}%)\")\n",
        "\n",
        "    # Run KMeans\n",
        "    # Choose K automatically via silhouette score (cosine). Fallback to NUM_CLUSTERS if needed\n",
        "    try:\n",
        "        selected_k, ks_scanned, sil_scores = choose_k_via_silhouette(embeddings_dedup, K_MIN, K_MAX)\n",
        "    except Exception:\n",
        "        selected_k, ks_scanned, sil_scores = max(1, min(NUM_CLUSTERS, int(embeddings_dedup.shape[0]))), [], []\n",
        "\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings_dedup, num_clusters=selected_k, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save under youtube_embeddings_project/deduplicated\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"deduplicated\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points_original\": int(original_count),\n",
        "        \"num_points_deduplicated\": int(unique_count),\n",
        "        \"duplicates_removed\": int(duplicate_count),\n",
        "        \"duplicate_percentage\": float(duplicate_percentage),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(selected_k),\n",
        "        \"silhouette_scan_ks\": ks_scanned,\n",
        "        \"silhouette_scores\": sil_scores,\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    for k in range(selected_k):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings_dedup, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids_dedup[idx].item() if hasattr(ids_dedup[idx], 'item') else ids_dedup[idx],\n",
        "                    \"comment\": comments_dedup[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{selected_k}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{selected_k}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_kmeans{selected_k}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nClustering complete.\")\n",
        "    print(f\"Summary written to: {summary_path}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2KRdIG3bQSm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize as l2_normalize\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script to:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Run KMeans into 10 clusters\n",
        "3) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "4) Save results as JSON files next to the .npz\n",
        "\n",
        "Notes:\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/7_Years_Lukas_Graham_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/7_Years_Lukas_Graham.json\"\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLUSTERS = 10\n",
        "NUM_CLOSEST = 1000\n",
        "NUM_FARTHEST = 1000\n",
        "\n",
        "# Performance and selection configuration\n",
        "K_MIN = 2\n",
        "K_MAX = 30  # upper bound; final range adapts to dataset size\n",
        "SIL_SUBSAMPLE = 8000  # number of points to sample for silhouette scoring\n",
        "USE_MINIBATCH_KMEANS = True\n",
        "MINIBATCH_SIZE = 4096\n",
        "PCA_DIM = 100  # set 0 to disable PCA\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    start = time.time()\n",
        "    if USE_MINIBATCH_KMEANS:\n",
        "        print(f\"[KMeans] Using MiniBatchKMeans: k={num_clusters}, batch_size={MINIBATCH_SIZE}\")\n",
        "        kmeans = MiniBatchKMeans(\n",
        "            n_clusters=num_clusters,\n",
        "            random_state=random_state,\n",
        "            batch_size=MINIBATCH_SIZE,\n",
        "            n_init=3,\n",
        "            reassignment_ratio=0.01,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"[KMeans] Using KMeans: k={num_clusters}\")\n",
        "        kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    print(f\"[KMeans] Fitted in {time.time() - start:.2f}s\")\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def choose_k_via_silhouette(\n",
        "    embeddings: np.ndarray,\n",
        "    k_min: int = K_MIN,\n",
        "    k_max: int = K_MAX,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[int, List[int], List[float]]:\n",
        "    \"\"\"Select k maximizing silhouette score with cosine distance.\n",
        "\n",
        "    Returns (selected_k, ks_scanned, silhouette_scores).\n",
        "    \"\"\"\n",
        "    n_samples = int(embeddings.shape[0])\n",
        "    if n_samples <= 2:\n",
        "        return 1, [1], [0.0]\n",
        "\n",
        "    # Adaptive k upper bound ~ sqrt(n) + 5\n",
        "    import math\n",
        "    k_min_eff = max(2, min(k_min, n_samples - 1))\n",
        "    k_max_eff = max(k_min_eff, min(int(math.sqrt(n_samples)) + 5, k_max, n_samples - 1))\n",
        "    print(f\"[Silhouette] Scanning k in [{k_min_eff}..{k_max_eff}] over n={n_samples} points\")\n",
        "\n",
        "    ks: List[int] = []\n",
        "    scores: List[float] = []\n",
        "    best_k = None\n",
        "    best_score = -1.0\n",
        "    # Subsample to speed up silhouette (O(N^2) distances)\n",
        "    rng = np.random.default_rng(seed=random_state)\n",
        "    if n_samples > SIL_SUBSAMPLE:\n",
        "        sample_indices = rng.choice(n_samples, size=SIL_SUBSAMPLE, replace=False)\n",
        "        X_sil = embeddings[sample_indices]\n",
        "        print(f\"[Silhouette] Using subsample size={len(sample_indices)} for scoring\")\n",
        "    else:\n",
        "        sample_indices = np.arange(n_samples)\n",
        "        X_sil = embeddings\n",
        "        print(f\"[Silhouette] Using full data for scoring (n={n_samples})\")\n",
        "\n",
        "    for k in range(k_min_eff, k_max_eff + 1):\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            if USE_MINIBATCH_KMEANS:\n",
        "                model = MiniBatchKMeans(\n",
        "                    n_clusters=k,\n",
        "                    random_state=random_state,\n",
        "                    batch_size=MINIBATCH_SIZE,\n",
        "                    n_init=3,\n",
        "                    reassignment_ratio=0.01,\n",
        "                )\n",
        "            else:\n",
        "                model = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
        "            labels_full = model.fit_predict(embeddings)\n",
        "            # Skip degenerate cases (single cluster label or any empty cluster)\n",
        "            if len(set(labels_full.tolist())) < 2:\n",
        "                continue\n",
        "            labels_sub = labels_full[sample_indices]\n",
        "            score = float(silhouette_score(X_sil, labels_sub, metric=\"cosine\"))\n",
        "            ks.append(k)\n",
        "            scores.append(score)\n",
        "            took = time.time() - t0\n",
        "            print(f\"[Silhouette] k={k:<3} score={score:>.4f} time={took:.2f}s\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if best_k is None:\n",
        "        return max(1, min(NUM_CLUSTERS, n_samples)), ([max(1, min(NUM_CLUSTERS, n_samples))]), ([0.0])\n",
        "    return int(best_k), ks, scores\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "\n",
        "    # Deduplicate embeddings (preserve first occurrence), keeping ids and comments aligned\n",
        "    # Use void view trick for row-wise uniqueness\n",
        "    print(\"[Dedup] Starting row-wise deduplication...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_view = embeddings.view(np.void)\n",
        "    _, unique_indices = np.unique(embeddings_view, axis=0, return_index=True)\n",
        "    unique_indices = np.sort(unique_indices)\n",
        "\n",
        "    original_count = int(embeddings.shape[0])\n",
        "    embeddings_dedup = embeddings[unique_indices]\n",
        "    ids_dedup = ids[unique_indices]\n",
        "    comments_dedup = [comments[i] for i in unique_indices]\n",
        "\n",
        "    unique_count = int(embeddings_dedup.shape[0])\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100 if original_count else 0.0\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"[Dedup] Removed {duplicate_count} duplicates ({duplicate_percentage:.1f}%) in {time.time() - t0:.2f}s\")\n",
        "    else:\n",
        "        print(f\"[Dedup] No duplicates found ({time.time() - t0:.2f}s)\")\n",
        "\n",
        "    # Normalize and optionally reduce dimensionality\n",
        "    print(\"[Preprocess] L2-normalizing vectors...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_proc = l2_normalize(embeddings_dedup, norm=\"l2\")\n",
        "    print(f\"[Preprocess] Normalized in {time.time() - t0:.2f}s\")\n",
        "\n",
        "    pca = None\n",
        "    if PCA_DIM and PCA_DIM > 0 and embeddings_proc.shape[1] > PCA_DIM:\n",
        "        print(f\"[Preprocess] Applying PCA to {PCA_DIM} dims from {embeddings_proc.shape[1]}...\")\n",
        "        t0 = time.time()\n",
        "        pca = PCA(n_components=PCA_DIM, random_state=42)\n",
        "        embeddings_proc = pca.fit_transform(embeddings_proc)\n",
        "        print(f\"[Preprocess] PCA completed in {time.time() - t0:.2f}s; new shape={embeddings_proc.shape}\")\n",
        "    else:\n",
        "        embeddings_proc = embeddings_proc\n",
        "        print(\"[Preprocess] PCA disabled or not needed\")\n",
        "\n",
        "    # Choose K automatically via silhouette score (cosine). Fallback to NUM_CLUSTERS if needed\n",
        "    try:\n",
        "        selected_k, ks_scanned, sil_scores = choose_k_via_silhouette(embeddings_proc, K_MIN, K_MAX)\n",
        "    except Exception:\n",
        "        selected_k, ks_scanned, sil_scores = max(1, min(NUM_CLUSTERS, int(embeddings_dedup.shape[0]))), [], []\n",
        "\n",
        "    print(f\"[Select-K] Selected k={selected_k}\")\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings_proc, num_clusters=selected_k, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save under youtube_embeddings_project/deduplicated\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"deduplicated2\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points_original\": int(original_count),\n",
        "        \"num_points_deduplicated\": int(unique_count),\n",
        "        \"duplicates_removed\": int(duplicate_count),\n",
        "        \"duplicate_percentage\": float(duplicate_percentage),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(selected_k),\n",
        "        \"silhouette_scan_ks\": ks_scanned,\n",
        "        \"silhouette_scores\": sil_scores,\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    for k in range(selected_k):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings_proc, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids_dedup[idx].item() if hasattr(ids_dedup[idx], 'item') else ids_dedup[idx],\n",
        "                    \"comment\": comments_dedup[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{selected_k}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_kmeans{selected_k}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_kmeans{selected_k}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nClustering complete.\")\n",
        "    print(f\"Summary written to: {summary_path}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6zQJEpD5PmU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "import time\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize as l2_normalize\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script using ELBOW METHOD for faster clustering:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Use elbow method to automatically select optimal k\n",
        "3) Run KMeans clustering\n",
        "4) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "5) Save results as JSON files\n",
        "\n",
        "Notes:\n",
        "- Elbow method is faster than silhouette (no pairwise distance computation)\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/Mockingbird_Eminem.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/Mockingbird_Eminem.json\"\n",
        "\n",
        "# Number of clusters and retrieval sizes\n",
        "NUM_CLOSEST = 100\n",
        "NUM_FARTHEST = 100\n",
        "\n",
        "# Performance and selection configuration\n",
        "K_MIN = 2\n",
        "K_MAX = 30  # upper bound; final range adapts to dataset size\n",
        "USE_MINIBATCH_KMEANS = True\n",
        "MINIBATCH_SIZE = 4096\n",
        "PCA_DIM = 0  # set 0 to disable PCA\n",
        "\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    start = time.time()\n",
        "    if USE_MINIBATCH_KMEANS:\n",
        "        print(f\"[KMeans] Using MiniBatchKMeans: k={num_clusters}, batch_size={MINIBATCH_SIZE}\")\n",
        "        kmeans = MiniBatchKMeans(\n",
        "            n_clusters=num_clusters,\n",
        "            random_state=random_state,\n",
        "            batch_size=MINIBATCH_SIZE,\n",
        "            n_init=3,\n",
        "            reassignment_ratio=0.01,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"[KMeans] Using KMeans: k={num_clusters}\")\n",
        "        kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    print(f\"[KMeans] Fitted in {time.time() - start:.2f}s\")\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def _compute_inertias_over_k(\n",
        "    embeddings: np.ndarray,\n",
        "    k_min: int,\n",
        "    k_max: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[List[int], List[float]]:\n",
        "    \"\"\"Compute inertias for different k values (faster than silhouette).\"\"\"\n",
        "    n_samples = int(embeddings.shape[0])\n",
        "    if n_samples <= 1:\n",
        "        return [1], [0.0]\n",
        "\n",
        "    # Adaptive k upper bound ~ sqrt(n) + 5\n",
        "    import math\n",
        "    k_min_eff = max(2, min(k_min, n_samples - 1))\n",
        "    k_max_eff = max(k_min_eff, min(int(math.sqrt(n_samples)) + 5, k_max, n_samples - 1))\n",
        "    print(f\"[Elbow] Scanning k in [{k_min_eff}..{k_max_eff}] over n={n_samples} points\")\n",
        "\n",
        "    ks: List[int] = []\n",
        "    inertias: List[float] = []\n",
        "\n",
        "    for k in range(k_min_eff, k_max_eff + 1):\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            if USE_MINIBATCH_KMEANS:\n",
        "                model = MiniBatchKMeans(\n",
        "                    n_clusters=k,\n",
        "                    random_state=random_state,\n",
        "                    batch_size=MINIBATCH_SIZE,\n",
        "                    n_init=3,\n",
        "                    reassignment_ratio=0.01,\n",
        "                )\n",
        "            else:\n",
        "                model = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
        "            model.fit(embeddings)\n",
        "            ks.append(k)\n",
        "            inertias.append(float(model.inertia_))\n",
        "            took = time.time() - t0\n",
        "            print(f\"[Elbow] k={k:<3} inertia={model.inertia_:>12.2f} time={took:.2f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Elbow] k={k} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not ks:\n",
        "        return [1], [0.0]\n",
        "    return ks, inertias\n",
        "\n",
        "\n",
        "def _select_k_by_elbow_distance(ks: List[int], inertias: List[float]) -> int:\n",
        "    \"\"\"Select k using maximum distance to line from first to last inertia point.\"\"\"\n",
        "    if len(ks) <= 1:\n",
        "        return ks[0]\n",
        "\n",
        "    # Distance from each point to the line between first and last\n",
        "    x1, y1 = ks[0], inertias[0]\n",
        "    x2, y2 = ks[-1], inertias[-1]\n",
        "    denom = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5 or 1.0\n",
        "\n",
        "    best_k = ks[0]\n",
        "    best_dist = -1.0\n",
        "\n",
        "    for x0, y0 in zip(ks, inertias):\n",
        "        # Perpendicular distance formula\n",
        "        num = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)\n",
        "        dist = num / denom\n",
        "        if dist > best_dist:\n",
        "            best_dist = dist\n",
        "            best_k = x0\n",
        "\n",
        "    print(f\"[Elbow] Selected k={best_k} (distance={best_dist:.2f})\")\n",
        "    return int(best_k)\n",
        "\n",
        "\n",
        "def choose_k_via_elbow(\n",
        "    embeddings: np.ndarray,\n",
        "    k_min: int = K_MIN,\n",
        "    k_max: int = K_MAX,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[int, List[int], List[float]]:\n",
        "    \"\"\"Select k using elbow method (faster than silhouette).\"\"\"\n",
        "    n_samples = int(embeddings.shape[0])\n",
        "    if n_samples <= 1:\n",
        "        return 1, [1], [0.0]\n",
        "\n",
        "    ks, inertias = _compute_inertias_over_k(embeddings, k_min, k_max, random_state)\n",
        "    selected_k = _select_k_by_elbow_distance(ks, inertias)\n",
        "    return selected_k, ks, inertias\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸš€ Starting clustering with ELBOW METHOD (faster than silhouette)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    print(\"[Path] Resolving embeddings path...\")\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "    print(f\"[Path] Found: {npz_path}\")\n",
        "\n",
        "    print(\"[Load] Loading embeddings and comments...\")\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "    print(f\"[Load] Loaded {embeddings.shape[0]:,} embeddings of dimension {embeddings.shape[1]}\")\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "    print(f\"[Load] Loaded {len(comments):,} comments\")\n",
        "\n",
        "    # Deduplicate embeddings (preserve first occurrence), keeping ids and comments aligned\n",
        "    print(\"[Dedup] Starting row-wise deduplication...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_view = embeddings.view(np.void)\n",
        "    _, unique_indices = np.unique(embeddings_view, axis=0, return_index=True)\n",
        "    unique_indices = np.sort(unique_indices)\n",
        "\n",
        "    original_count = int(embeddings.shape[0])\n",
        "    embeddings_dedup = embeddings[unique_indices]\n",
        "    ids_dedup = ids[unique_indices]\n",
        "    comments_dedup = [comments[i] for i in unique_indices]\n",
        "\n",
        "    unique_count = int(embeddings_dedup.shape[0])\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100 if original_count else 0.0\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"[Dedup] Removed {duplicate_count} duplicates ({duplicate_percentage:.1f}%) in {time.time() - t0:.2f}s\")\n",
        "    else:\n",
        "        print(f\"[Dedup] No duplicates found ({time.time() - t0:.2f}s)\")\n",
        "\n",
        "    # Normalize and optionally reduce dimensionality\n",
        "    print(\"[Preprocess] L2-normalizing vectors...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_proc = l2_normalize(embeddings_dedup, norm=\"l2\")\n",
        "    print(f\"[Preprocess] Normalized in {time.time() - t0:.2f}s\")\n",
        "\n",
        "    pca = None\n",
        "    if PCA_DIM and PCA_DIM > 0 and embeddings_proc.shape[1] > PCA_DIM:\n",
        "        print(f\"[Preprocess] Applying PCA to {PCA_DIM} dims from {embeddings_proc.shape[1]}...\")\n",
        "        t0 = time.time()\n",
        "        pca = PCA(n_components=PCA_DIM, random_state=42)\n",
        "        embeddings_proc = pca.fit_transform(embeddings_proc)\n",
        "        print(f\"[Preprocess] PCA completed in {time.time() - t0:.2f}s; new shape={embeddings_proc.shape}\")\n",
        "    else:\n",
        "        print(\"[Preprocess] PCA disabled or not needed\")\n",
        "\n",
        "    # Choose K automatically via elbow method (faster than silhouette)\n",
        "    print(\"\\n[Elbow] Starting k selection via elbow method...\")\n",
        "    try:\n",
        "        selected_k, ks_scanned, inertias = choose_k_via_elbow(embeddings_proc, K_MIN, K_MAX)\n",
        "    except Exception as e:\n",
        "        print(f\"[Elbow] Error in elbow method: {e}\")\n",
        "        selected_k = max(1, min(10, int(embeddings_proc.shape[0])))\n",
        "        ks_scanned, inertias = [selected_k], [0.0]\n",
        "\n",
        "    print(f\"[Select-K] Selected k={selected_k}\")\n",
        "\n",
        "    # Run final clustering\n",
        "    print(f\"\\n[Cluster] Running final clustering with k={selected_k}...\")\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings_proc, num_clusters=selected_k, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save under youtube_embeddings_project/deduplicated\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"deduplicated5\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points_original\": int(original_count),\n",
        "        \"num_points_deduplicated\": int(unique_count),\n",
        "        \"duplicates_removed\": int(duplicate_count),\n",
        "        \"duplicate_percentage\": float(duplicate_percentage),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(selected_k),\n",
        "        \"elbow_scan_ks\": ks_scanned,\n",
        "        \"elbow_inertias\": inertias,\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[Output] Writing cluster results...\")\n",
        "    for k in range(selected_k):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings_proc, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids_dedup[idx].item() if hasattr(ids_dedup[idx], 'item') else ids_dedup[idx],\n",
        "                    \"comment\": comments_dedup[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_elbow_kmeans{selected_k}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_elbow_kmeans{selected_k}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_elbow_kmeans{selected_k}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… ELBOW METHOD CLUSTERING COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“Š Clustered {unique_count:,} deduplicated embeddings into {selected_k} clusters\")\n",
        "    print(f\"ðŸ“ Summary written to: {summary_path}\")\n",
        "    print(f\"ðŸ“‚ Output directory: {out_dir}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gF5nj2PCTrGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "import time\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Optional: Google Colab drive mounting\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    drive = None  # type: ignore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize as l2_normalize\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Colab-ready script with FIXED K=3 for maximum speed:\n",
        "1) Load embeddings from an .npz file (expects keys: 'embeddings', 'ids')\n",
        "2) Use fixed k=3 (no k selection needed - fastest option)\n",
        "3) Run KMeans clustering\n",
        "4) For each cluster, extract:\n",
        "   - 100 closest comments to the centroid\n",
        "   - 500 farthest comments from the centroid\n",
        "5) Save results as JSON files\n",
        "\n",
        "Notes:\n",
        "- Fixed k=3 eliminates k selection overhead - fastest possible clustering\n",
        "- Actual comments are not stored inside the .npz in this project. This script will\n",
        "  try to find the source JSON (same stem as the .npz, without the '_embeddings' suffix)\n",
        "  in the same directory. If it is elsewhere, set COMMENTS_JSON_PATH below.\n",
        "- The source JSON can be either a list of strings or a list of objects with\n",
        "  fields {'id': ..., 'comment': ...}. When 'id' is missing, positional index is used.\n",
        "\n",
        "Usage in Colab:\n",
        "- Just run this file content in a cell (or upload as a .py and run it). No CLI needed.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ==== USER CONFIG (edit these two lines as needed) ====\n",
        "EMBED_NPZ_PATH = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/embeddings/Without_Me_Eminem_embeddings.npz\"\n",
        "\n",
        "# If None, the script will try to infer the JSON path from the .npz filename.\n",
        "COMMENTS_JSON_PATH: Optional[str] = \\\n",
        "    \"/content/drive/My Drive/youtube_embeddings_project/comments/Without_Me_Eminem_embeddings.json\"\n",
        "\n",
        "# Fixed clustering configuration\n",
        "FIXED_K = 3  # Fixed number of clusters\n",
        "NUM_CLOSEST = 1000\n",
        "NUM_FARTHEST = 1000\n",
        "\n",
        "# Performance configuration\n",
        "USE_MINIBATCH_KMEANS = False  # Use regular KMeans for better results\n",
        "\n",
        "\n",
        "def _maybe_mount_drive():\n",
        "    try:\n",
        "        if drive is not None and not os.path.isdir(\"/content/drive/MyDrive\") and not os.path.isdir(\"/content/drive/My Drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _default_embed_dirs() -> List[str]:\n",
        "    \"\"\"Return possible embeddings directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/embeddings\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/embeddings\",\n",
        "        \"./embeddings\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def _default_comment_dirs() -> List[str]:\n",
        "    \"\"\"Return possible comments directories (handles MyDrive/My Drive and local).\"\"\"\n",
        "    return [\n",
        "        \"/content/drive/MyDrive/youtube_embeddings_project/comments\",\n",
        "        \"/content/drive/My Drive/youtube_embeddings_project/comments\",\n",
        "        \"./comments\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_npz_embeddings(npz_path: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    if 'embeddings' not in data or 'ids' not in data:\n",
        "        raise KeyError(\".npz must contain 'embeddings' and 'ids' arrays\")\n",
        "\n",
        "    embeddings: np.ndarray = data['embeddings']\n",
        "    ids: np.ndarray = data['ids']\n",
        "    meta = {\"keys\": list(data.keys())}\n",
        "    return embeddings, ids, meta\n",
        "\n",
        "\n",
        "def infer_comments_json_path(npz_path: str) -> Optional[str]:\n",
        "    \"\"\"Try to infer the comments JSON path for a given .npz.\n",
        "\n",
        "    Search order:\n",
        "    1) Same directory as .npz with base name (stripping _embeddings) and .json\n",
        "    2) Known comments directories with exact filename match\n",
        "    3) Fuzzy match by normalized stem across known comments directories\n",
        "    \"\"\"\n",
        "    p = Path(npz_path)\n",
        "    stem = p.stem\n",
        "    # Expect pattern like XYZ_embeddings.npz -> XYZ.json\n",
        "    if stem.endswith(\"_embeddings\"):\n",
        "        base = stem[:-len(\"_embeddings\")]\n",
        "    else:\n",
        "        base = stem\n",
        "\n",
        "    # 1) Same directory\n",
        "    candidate = p.parent / f\"{base}.json\"\n",
        "    if candidate.exists():\n",
        "        return str(candidate)\n",
        "\n",
        "    # 2) Exact match in known comments dirs\n",
        "    _maybe_mount_drive()\n",
        "    comment_dirs = _default_comment_dirs()\n",
        "    for d in comment_dirs:\n",
        "        cand = Path(d) / f\"{base}.json\"\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "\n",
        "    # 3) Fuzzy match across known comments dirs\n",
        "    norm_target = _normalize_name(base)\n",
        "    all_jsons: List[str] = []\n",
        "    for d in comment_dirs:\n",
        "        all_jsons.extend(sorted(glob(os.path.join(d, \"*.json\"))))\n",
        "\n",
        "    if not all_jsons:\n",
        "        return None\n",
        "\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for path in all_jsons:\n",
        "        cnorm = _normalize_name(Path(path).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, path))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score >= 0.5:\n",
        "        return best_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def resolve_npz_path(path_or_stem: str) -> str:\n",
        "    \"\"\"Resolve an embeddings .npz path from an exact path or a song stem.\n",
        "\n",
        "    Strategy:\n",
        "    1) If path exists, return it.\n",
        "    2) If it's just a filename (endswith .npz), try in DEFAULT_EMBED_DIR.\n",
        "    3) Fuzzy search in DEFAULT_EMBED_DIR using normalized containment and difflib.\n",
        "    \"\"\"\n",
        "    # 1) Direct hit\n",
        "    if os.path.isfile(path_or_stem):\n",
        "        return path_or_stem\n",
        "\n",
        "    # 2) Try as filename inside default dir\n",
        "    if path_or_stem.endswith(\".npz\"):\n",
        "        candidate = os.path.join(DEFAULT_EMBED_DIR, os.path.basename(path_or_stem))\n",
        "        if os.path.isfile(candidate):\n",
        "            return candidate\n",
        "\n",
        "    # 3) Fuzzy search by stem across known directories\n",
        "    stem = Path(path_or_stem).stem\n",
        "    norm_target = _normalize_name(stem)\n",
        "\n",
        "    # Try to ensure Drive is mounted before searching\n",
        "    _maybe_mount_drive()\n",
        "\n",
        "    all_candidates: List[str] = []\n",
        "    searched_dirs: List[str] = []\n",
        "    for d in _default_embed_dirs():\n",
        "        searched_dirs.append(d)\n",
        "        all_candidates.extend(sorted(glob(os.path.join(d, \"*.npz\"))))\n",
        "\n",
        "    if not all_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No .npz files found in any of: \" + \", \".join(searched_dirs)\n",
        "        )\n",
        "\n",
        "    # Score candidates\n",
        "    scored: List[Tuple[float, str]] = []\n",
        "    for c in all_candidates:\n",
        "        cnorm = _normalize_name(Path(c).stem)\n",
        "        if norm_target and norm_target in cnorm:\n",
        "            score = 1.0\n",
        "        else:\n",
        "            score = difflib.SequenceMatcher(None, norm_target, cnorm).ratio()\n",
        "        scored.append((score, c))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_path = scored[0]\n",
        "    if best_score < 0.4:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not resolve embeddings .npz for '{path_or_stem}'. Best match score={best_score:.2f} from dirs: \"\n",
        "            + \", \".join(searched_dirs)\n",
        "        )\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def load_comments(ids: np.ndarray, comments_json_path: Optional[str]) -> List[str]:\n",
        "    if comments_json_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not determine source comments JSON. Set COMMENTS_JSON_PATH explicitly.\")\n",
        "\n",
        "    with open(comments_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build mapping from id -> comment\n",
        "    comments_by_id: Dict[Any, str] = {}\n",
        "\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict) and 'comment' in data[0]:\n",
        "            for item in data:\n",
        "                cid = item.get('id')\n",
        "                if cid is None:\n",
        "                    # Fall back to positional index if id missing\n",
        "                    # We cannot recover original index reliably unless we add one; so skip\n",
        "                    # positional fallback handled below\n",
        "                    pass\n",
        "                else:\n",
        "                    comments_by_id[cid] = item.get('comment', \"\")\n",
        "        elif isinstance(data[0], str):\n",
        "            # Positional mapping only\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported JSON format for comments.\")\n",
        "    else:\n",
        "        raise ValueError(\"Comments JSON is empty or invalid.\")\n",
        "\n",
        "    resolved_comments: List[str] = []\n",
        "    if comments_by_id:\n",
        "        for cid in ids:\n",
        "            resolved_comments.append(comments_by_id.get(cid, \"\"))\n",
        "    else:\n",
        "        # Positional fallback: assume ids are 0..N-1 indices\n",
        "        if not isinstance(data[0], str):\n",
        "            raise ValueError(\"Cannot map ids to comments: ids present but JSON lacks 'id' fields; and data is not a list of strings for positional mapping.\")\n",
        "        for cid in ids:\n",
        "            try:\n",
        "                resolved_comments.append(data[int(cid)])\n",
        "            except Exception:\n",
        "                resolved_comments.append(\"\")\n",
        "\n",
        "    return resolved_comments\n",
        "\n",
        "\n",
        "def compute_cluster_members(\n",
        "    embeddings: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, KMeans]:\n",
        "    start = time.time()\n",
        "    print(f\"[KMeans] Using KMeans: k={num_clusters}\")\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=random_state)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    print(f\"[KMeans] Fitted in {time.time() - start:.2f}s\")\n",
        "    return labels, centers, kmeans\n",
        "\n",
        "\n",
        "def select_nearest_and_farthest(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    centers: np.ndarray,\n",
        "    cluster_index: int,\n",
        "    num_closest: int,\n",
        "    num_farthest: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    members = np.where(labels == cluster_index)[0]\n",
        "    if members.size == 0:\n",
        "        return members, np.array([], dtype=int), np.array([], dtype=int)\n",
        "\n",
        "    center = centers[cluster_index]\n",
        "    member_vectors = embeddings[members]\n",
        "    dists = np.linalg.norm(member_vectors - center, axis=1)\n",
        "\n",
        "    order = np.argsort(dists)\n",
        "    closest = members[order[: min(num_closest, members.size)]]\n",
        "    farthest = members[order[::-1][: min(num_farthest, members.size)]]\n",
        "    return members, closest, farthest\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸš€ Starting clustering with FIXED K=3 (maximum speed)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Resolve path robustly (supports exact path, filename, or fuzzy stem)\n",
        "    print(\"[Path] Resolving embeddings path...\")\n",
        "    npz_path = resolve_npz_path(EMBED_NPZ_PATH)\n",
        "    print(f\"[Path] Found: {npz_path}\")\n",
        "\n",
        "    print(\"[Load] Loading embeddings and comments...\")\n",
        "    embeddings, ids, meta = load_npz_embeddings(npz_path)\n",
        "    print(f\"[Load] Loaded {embeddings.shape[0]:,} embeddings of dimension {embeddings.shape[1]}\")\n",
        "\n",
        "    # Resolve comments JSON path\n",
        "    comments_path = COMMENTS_JSON_PATH or infer_comments_json_path(npz_path)\n",
        "    comments = load_comments(ids, comments_path)\n",
        "    print(f\"[Load] Loaded {len(comments):,} comments\")\n",
        "\n",
        "    # Deduplicate embeddings (preserve first occurrence), keeping ids and comments aligned\n",
        "    print(\"[Dedup] Starting row-wise deduplication...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_view = embeddings.view(np.void)\n",
        "    _, unique_indices = np.unique(embeddings_view, axis=0, return_index=True)\n",
        "    unique_indices = np.sort(unique_indices)\n",
        "\n",
        "    original_count = int(embeddings.shape[0])\n",
        "    embeddings_dedup = embeddings[unique_indices]\n",
        "    ids_dedup = ids[unique_indices]\n",
        "    comments_dedup = [comments[i] for i in unique_indices]\n",
        "\n",
        "    unique_count = int(embeddings_dedup.shape[0])\n",
        "    duplicate_count = original_count - unique_count\n",
        "    duplicate_percentage = (duplicate_count / original_count) * 100 if original_count else 0.0\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"[Dedup] Removed {duplicate_count} duplicates ({duplicate_percentage:.1f}%) in {time.time() - t0:.2f}s\")\n",
        "    else:\n",
        "        print(f\"[Dedup] No duplicates found ({time.time() - t0:.2f}s)\")\n",
        "\n",
        "    # Normalize vectors (improves cosine-like behavior for Euclidean KMeans)\n",
        "    print(\"[Preprocess] L2-normalizing vectors...\")\n",
        "    t0 = time.time()\n",
        "    embeddings_proc = l2_normalize(embeddings_dedup, norm=\"l2\")\n",
        "    print(f\"[Preprocess] Normalized in {time.time() - t0:.2f}s\")\n",
        "    print(f\"[Preprocess] Using full {embeddings_proc.shape[1]} dimensions (no PCA)\")\n",
        "\n",
        "    # Use fixed k=3 (no k selection needed - fastest option)\n",
        "    selected_k = FIXED_K\n",
        "    print(f\"[Config] Using fixed k={selected_k} (no k selection overhead)\")\n",
        "\n",
        "    # Ensure we have enough data for k clusters\n",
        "    if unique_count < selected_k:\n",
        "        print(f\"[Warning] Only {unique_count} unique embeddings, reducing k from {selected_k} to {unique_count}\")\n",
        "        selected_k = unique_count\n",
        "\n",
        "    # Run clustering directly\n",
        "    print(f\"\\n[Cluster] Running clustering with fixed k={selected_k}...\")\n",
        "    labels, centers, kmeans = compute_cluster_members(\n",
        "        embeddings_proc, num_clusters=selected_k, random_state=42\n",
        "    )\n",
        "\n",
        "    # Verify the number of clusters actually created\n",
        "    actual_clusters = len(np.unique(labels))\n",
        "    print(f\"[Cluster] Created {actual_clusters} clusters (expected {selected_k})\")\n",
        "    if actual_clusters != selected_k:\n",
        "        print(f\"[Warning] Expected {selected_k} clusters but got {actual_clusters}\")\n",
        "        selected_k = actual_clusters\n",
        "\n",
        "    # Save under youtube_embeddings_project/deduplicated\n",
        "    embeddings_dir = Path(npz_path).parent\n",
        "    project_dir = embeddings_dir.parent\n",
        "    out_dir_path = project_dir / \"deduplicated10\"\n",
        "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    out_dir = str(out_dir_path)\n",
        "    base_name = Path(npz_path).stem.replace(\"_embeddings\", \"\")\n",
        "\n",
        "    summary: Dict[str, Any] = {\n",
        "        \"npz_path\": npz_path,\n",
        "        \"comments_json_path\": comments_path,\n",
        "        \"num_points_original\": int(original_count),\n",
        "        \"num_points_deduplicated\": int(unique_count),\n",
        "        \"duplicates_removed\": int(duplicate_count),\n",
        "        \"duplicate_percentage\": float(duplicate_percentage),\n",
        "        \"embedding_dim\": int(embeddings.shape[1]),\n",
        "        \"num_clusters\": int(selected_k),\n",
        "        \"clustering_method\": \"fixed_k\",\n",
        "        \"per_cluster_counts\": {},\n",
        "        \"outputs\": [],\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[Output] Writing cluster results...\")\n",
        "    for k in range(selected_k):\n",
        "        members, closest, farthest = select_nearest_and_farthest(\n",
        "            embeddings_proc, labels, centers, k, NUM_CLOSEST, NUM_FARTHEST\n",
        "        )\n",
        "\n",
        "        print(f\"[Cluster {k}] Members: {members.size}, Closest: {len(closest)}, Farthest: {len(farthest)}\")\n",
        "\n",
        "        # Prepare outputs\n",
        "        def pack(indices: np.ndarray) -> List[Dict[str, Any]]:\n",
        "            result: List[Dict[str, Any]] = []\n",
        "            for idx in indices.tolist():\n",
        "                result.append({\n",
        "                    \"index\": int(idx),\n",
        "                    \"id\": ids_dedup[idx].item() if hasattr(ids_dedup[idx], 'item') else ids_dedup[idx],\n",
        "                    \"comment\": comments_dedup[idx],\n",
        "                })\n",
        "            return result\n",
        "\n",
        "        closest_payload = pack(closest)\n",
        "        farthest_payload = pack(farthest)\n",
        "\n",
        "        # Write per-cluster files\n",
        "        closest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_fixed_k{selected_k}_cluster_{k}_nearest_{len(closest_payload)}.json\"\n",
        "        )\n",
        "        farthest_path = os.path.join(\n",
        "            out_dir, f\"{base_name}_fixed_k{selected_k}_cluster_{k}_farthest_{len(farthest_payload)}.json\"\n",
        "        )\n",
        "\n",
        "        with open(closest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(closest_payload, f, ensure_ascii=False, indent=2)\n",
        "        with open(farthest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(farthest_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        summary[\"per_cluster_counts\"][str(k)] = {\n",
        "            \"members\": int(members.size),\n",
        "            \"closest_saved\": int(len(closest_payload)),\n",
        "            \"farthest_saved\": int(len(farthest_payload)),\n",
        "        }\n",
        "        summary[\"outputs\"].append({\n",
        "            \"cluster\": k,\n",
        "            \"closest_path\": closest_path,\n",
        "            \"farthest_path\": farthest_path,\n",
        "        })\n",
        "\n",
        "    # Write summary\n",
        "    summary_path = os.path.join(out_dir, f\"{base_name}_fixed_k{selected_k}_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… FIXED K=3 CLUSTERING COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“Š Clustered {unique_count:,} deduplicated embeddings into {selected_k} clusters\")\n",
        "    print(f\"âš¡ Used fixed k={selected_k} (no k selection overhead)\")\n",
        "    print(f\"ðŸ“ Summary written to: {summary_path}\")\n",
        "    print(f\"ðŸ“‚ Output directory: {out_dir}\")\n",
        "    for entry in summary[\"outputs\"]:\n",
        "        print(f\"Cluster {entry['cluster']:>2}:\\n  nearest -> {entry['closest_path']}\\n  farthest -> {entry['farthest_path']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "V543Zrr3buSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}