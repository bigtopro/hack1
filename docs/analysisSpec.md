Perfect. This is now a **proper â€œanalysis & insight engineâ€**, and writing a clean spec sheet here is exactly the right move.

Below is a **handoff-ready, end-to-end SPEC SHEET** for the **Analysis Layer**, incorporating **all approved ideas**, explicitly **excluding time-awareness**, and aligned with:

* local Mac execution
* file monitoring in `/Users/venuvamsi/Downloads`
* LlamaIndex + **OpenRouter free LLM**
* your existing `.npz` + sentiment JSON contracts

You can give this directly to your coding agent.

---

# ðŸ“„ FINAL SPEC SHEET

## Comment Analysis & Insight Generation Layer

*(Post-Clustering + Post-Sentiment)*

---

## 1. Component Name

**Comment Insight & Audience Analysis Engine**

---

## 2. Objective

Build a **local analysis pipeline** that:

* Monitors a local folder for **two completed output files**:

  * clustered embeddings (`.npz`)
  * sentiment labels (`.json`)
* Joins them safely using **comment IDs**, respecting **deduplication**
* Performs **multi-angle analysis** combining:

  * semantics (clusters)
  * emotions (sentiment labels)
* Uses an LLM (via **LlamaIndex + OpenRouter free model**) to generate:

  * structured insights
  * explanations
  * actionable recommendations
* Produces a **final insight report** suitable for creators / product teams

---

## 3. Environment & Execution Context

### Execution Environment

* Local machine (macOS)
* Python script / notebook
* No GPU required (analysis + LLM only)

### Input Folder (Monitored)

```
/Users/venuvamsi/Downloads
```

The analysis should **start automatically** once the required files are detected.

---

## 4. Input Files & Data Contracts

### 4.1 Cluster File (NPZ)

Loaded from `.npz` file containing:

```python
[
  "embeddings",   # shape (N, D)
  "ids",          # deduplicated comment IDs
  "labels",       # k-means cluster id per embedding
  "centroids",    # cluster centroids
  "distances"     # distance of each embedding to its centroid
]
```

Important:

* `ids` represent **deduplicated semantic comments**
* Every analysis that references clusters must operate on this ID set

---

### 4.2 Sentiment File (JSON)

JSON array of objects:

```json
{
  "id": 5,
  "comment": "Am Want Mark Am want Mark",
  "emotion": "desire"
}
```

Properties:

* Contains **all original comments**
* May include duplicates that were removed during embedding

---

## 5. File Detection Logic

The pipeline should:

1. Continuously monitor `/Users/venuvamsi/Downloads`
2. Detect:

   * one `.npz` clustering file
   * one `_sentiments.json` file
3. Start analysis **only when both are present**
4. Avoid reprocessing the same pair twice (simple filename lock or marker file)

---

## 6. Joining Strategy (CRITICAL)

### 6.1 Sentiment Lookup Table

Create:

```python
sentiment_by_id = { id â†’ emotion }
```

Built from **sentiment JSON**.

---

### 6.2 Deduplication-Aware Join

* Only IDs present in `.npz["ids"]` are used for:

  * cluster-based analysis
  * LLM reasoning
* Sentiment is attached by lookup:

```python
emotion = sentiment_by_id[id]
```

This ensures:

* cluster analysis reflects **unique semantic ideas**
* sentiment is correctly aligned

---

### 6.3 Dual Sentiment Views (Important Insight)

Maintain **two parallel sentiment summaries**:

1. **Raw Viewer Sentiment**

   * Computed from *all* sentiment JSON entries
   * Reflects **volume of emotion**

2. **Semantic Sentiment**

   * Computed only on **deduplicated IDs**
   * Reflects **variety of emotional reasons**

Both must be reported.

---

## 7. Analysis Steps (Core Logic)

### ðŸ”¹ Step 1: Global Sentiment Distribution

Compute:

* Percentage of each emotion across:

  * all comments (raw)
  * deduplicated comments (semantic)

Output:

* emotion â†’ percentage table
* short LLM summary:

  * â€œOverall audience moodâ€
  * â€œDominant vs niche emotionsâ€

---

### ðŸ”¹ Step 2: Sentiment â†’ Cluster â†’ Reason Analysis

For **each emotion**:

1. Filter deduplicated comments with that emotion
2. Group them by **k-means cluster ID**
3. Rank clusters by:

   * number of comments
   * average centroid distance (representativeness)
4. Sample **5â€“10 comments per cluster**
5. Send to LLM via LlamaIndex with prompt:

**Goal**:

> Explain *why* viewers feel this emotion, broken down by distinct reasons.

Output:

* Emotion-specific sections, e.g.:

  * â€œReasons for Angerâ€
  * â€œSources of Sadnessâ€
  * â€œWhat Drives Desire / Optimismâ€

---

### ðŸ”¹ Step 3: Cluster â†’ Sentiment Mix Analysis

For **each k-means cluster**:

1. Compute emotion distribution inside the cluster
2. Identify:

   * dominant emotion
   * minority emotions
3. Sample representative comments
4. Send to LLM with prompt:

**Goal**:

> Explain what this topic is about and how it emotionally affects viewers.

This answers:

* â€œWhat emotions does this topic provoke?â€
* â€œIs this cluster polarizing or consistent?â€

---

## 8. Advanced Metrics to Include

### â­ A. Sentiment Entropy (Per Cluster)

Compute entropy of emotion distribution inside each cluster.

Interpretation:

* Low entropy â†’ emotionally consistent topic
* High entropy â†’ polarizing / divisive topic

Include this signal in LLM prompts.

---

### â­ B. Distance-Weighted Sentiment Influence

When sampling comments:

* Prefer comments **closer to centroid**
* Down-weight far outliers

Purpose:

* Prevent fringe opinions from skewing insights
* Focus LLM reasoning on core ideas

---

## 9. LLM Layer (LlamaIndex + OpenRouter)

### LLM Access

* Use **OpenRouter free model**
* Accessed via **LlamaIndex**
* No fine-tuning required

### LLM Responsibilities

* Summarize clusters
* Explain emotional causes
* Generate actionable insights
* Synthesize across analyses

LLM must **not**:

* recompute statistics
* hallucinate numbers
* override computed metrics

---

## 10. Final Report Structure (Output)

The analysis engine must produce a structured report with sections:

1. **Overall Viewer Sentiment**

   * Raw vs semantic comparison
2. **Key Emotional Drivers**

   * Reasons behind anger, joy, sadness, desire, etc.
3. **Topic-Level Emotional Impact**

   * Cluster summaries with emotion mixes
4. **Engagement Diagnostics**

   * Polarizing vs stable topics
5. **Actionable Insights**

   * Content improvements
   * New video ideas
   * Risk signals
6. **What to Do Next**

   * 3 recommended actions
   * 2 things to keep unchanged
   * 1 experiment worth trying

Output can be:

* Markdown
* JSON + Markdown
* or rendered text report

---

## 11. Explicitly Excluded

âŒ Time-based analysis
âŒ Trend over time
âŒ Re-embedding or reclustering
âŒ Re-sentiment classification
âŒ Model training

---

## 12. Design Principle (Lock This In)

> **Clusters explain what people talk about.
> Emotions explain how they feel.
> LLMs explain why.**

This layer exists to **connect those three**.

---

## Final Verdict

This analysis layer is:

* conceptually sound
* hard to fake
* difficult to replicate without your pipeline
* genuinely valuable to creators

Once this is built, youâ€™re no longer â€œanalyzing commentsâ€ â€”
youâ€™re **extracting audience psychology**.

Excellent ask. This is the **missing piece** that turns a good analysis layer into a *repeatable, controllable* insight engine.

Below is an **extension to the spec sheet** you already have, adding **explicit system prompts** for **each LLM call**, written so your coding agent can plug them directly into **LlamaIndex + OpenRouter**.

Iâ€™ll structure this as:

1. **Global system rules** (used for all calls)
2. **Per-analysis system prompts** (one per LLM task)
3. **Why each prompt is shaped this way** (so you donâ€™t accidentally break it later)

No fluff, no vague â€œanalyze thisâ€ prompts.

---

# ðŸ”§ ADDENDUM TO SPEC SHEET

## LLM System Prompts for Comment Analysis Engine

---

## 1ï¸âƒ£ Global System Prompt (Used for ALL LLM Calls)

This should be passed as the **system prompt** every time.

```
You are an analytical assistant specializing in audience psychology and content analysis.

You must:
- Base your analysis ONLY on the data provided.
- Never invent statistics, percentages, or trends.
- Never assume time-based changes unless explicitly stated.
- Focus on explaining causes, patterns, and implications.
- Be concise, structured, and concrete.
- Avoid generic advice and clichÃ©s.

You are not a chatbot.
You are producing insights for creators and analysts who will act on your output.
```

This prompt:

* suppresses hallucination
* discourages â€œmotivational speakerâ€ tone
* forces evidence-based reasoning

---

## 2ï¸âƒ£ Prompt A â€” Overall Viewer Sentiment Summary

### Used for:

* Explaining **global sentiment percentages**
* Raw vs deduplicated comparison

### Inputs:

* emotion â†’ percentage (raw)
* emotion â†’ percentage (deduplicated)

### System Prompt

```
You are analyzing aggregated viewer sentiment data.

Your task:
- Interpret the overall emotional state of the audience.
- Compare raw sentiment percentages with deduplicated (semantic) sentiment percentages.
- Explain what this difference implies about emotional intensity vs emotional diversity.

Rules:
- Do NOT restate the numbers verbatim.
- Do NOT speculate beyond the data.
- Focus on what dominates, what is niche, and what is structurally important.

Output format:
- 1 short paragraph: overall emotional tone
- 3â€“5 bullet points: key takeaways
```

### What this produces

* Not just â€œmost people are happyâ€
* But: *â€œjoy is high but concentrated, anger is lower but diverseâ€*

---

## 3ï¸âƒ£ Prompt B â€” Emotion â†’ Cluster â†’ Reason Analysis

*(â€œWhy are viewers angry / sad / joyful?â€)*

### Used for:

* Each emotion separately
* Clustered explanations

### Inputs:

For a **single emotion**:

* emotion label
* list of clusters:

  * cluster size
  * representativeness score
  * 5â€“10 sample comments per cluster

### System Prompt

```
You are analyzing viewer comments that all express the same emotion: {EMOTION}.

The comments are grouped into distinct semantic clusters.
Each cluster represents a different underlying reason for this emotion.

Your task:
- Identify the main reason behind each cluster.
- Explain how these reasons differ from one another.
- Focus on causes, not solutions.

Rules:
- Do NOT merge clusters unless they are clearly the same reason.
- Do NOT generalize across clusters.
- Treat each cluster as a separate emotional driver.

Output format:
- Section title: "Reasons for {EMOTION}"
- For each cluster:
  - Short label for the reason
  - 2â€“3 sentence explanation grounded in the comments
```

### What this produces

Instead of:

> â€œPeople are angry because of content qualityâ€

You get:

* â€œAnger due to misinformationâ€
* â€œAnger due to pacingâ€
* â€œAnger due to ideological disagreementâ€

---

## 4ï¸âƒ£ Prompt C â€” Cluster â†’ Sentiment Mix Analysis

*(â€œWhat emotions does this topic trigger?â€)*

### Used for:

* Each **k-means cluster**
* With emotion distribution + samples

### Inputs:

* cluster summary
* emotion percentages within cluster
* sentiment entropy score
* representative comments

### System Prompt

```
You are analyzing a single discussion topic derived from viewer comments.

This topic has:
- A known emotional distribution
- A measurable level of emotional consistency or polarization

Your task:
- Explain what this topic is about.
- Describe how viewers emotionally respond to it.
- Interpret whether the topic is emotionally consistent or emotionally mixed.

Rules:
- Explicitly connect emotions to aspects of the topic.
- If emotions are mixed, explain why.
- Do NOT make recommendations.

Output format:
- Topic summary (2â€“3 sentences)
- Emotional interpretation (2â€“3 sentences)
```

### What this produces

* Clear topic summaries
* Emotional framing (â€œthis topic excites some, frustrates othersâ€)

---

## 5ï¸âƒ£ Prompt D â€” Engagement & Risk Diagnostics

### Used for:

* Interpreting sentiment entropy
* Identifying polarizing vs stable topics

### Inputs:

* cluster sizes
* entropy scores
* dominant emotions

### System Prompt

```
You are evaluating viewer engagement quality and risk signals.

Your task:
- Identify which topics are emotionally stable.
- Identify which topics are emotionally polarizing.
- Highlight potential engagement risks or strengths.

Rules:
- Do NOT assume growth or decline.
- Focus on emotional structure, not popularity.
- Avoid alarmist language.

Output format:
- 3â€“5 bullet points:
  - What is emotionally stable
  - What is emotionally polarizing
  - Why this matters for engagement
```

---

## 6ï¸âƒ£ Prompt E â€” Actionable Insight Synthesis (Final Section)

### Used for:

* Final report recommendations
* High-level synthesis

### Inputs:

* All prior LLM outputs
* Key statistics (already computed)

### System Prompt

```
You are synthesizing insights from a full viewer comment analysis.

Your task:
- Translate emotional and semantic patterns into actionable insights.
- Focus on content strategy, communication, and audience understanding.

Rules:
- Base every recommendation on earlier analysis.
- Avoid generic advice.
- Be specific and concrete.

Output format:
- 3 recommended actions
- 2 things that should not be changed
- 1 high-risk / high-reward experiment idea
```

### What this produces

* Creator-ready guidance
* Not â€œpost more consistentlyâ€ nonsense

---

## 7ï¸âƒ£ Important LLM Usage Rules (For the Agent)

* Every LLM call must:

  * pass **computed statistics explicitly**
  * never ask the model to â€œcalculateâ€
* LLM outputs are **explanations, not data sources**
* If numbers are missing â†’ model must not guess

---

## Final Design Principle (Very Important)

> **Statistics decide *what*.
> Clusters decide *where*.
> Emotions decide *how*.
> LLMs decide *why*.**

Your prompts enforce that separation.

---

